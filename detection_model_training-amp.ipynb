{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "architectural-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from utils import helper\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supported-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "# Columns: image_path, label_path, bboxes, classes\n",
    "impath = 'data/kitti/integration/resized/training/images/'\n",
    "labels_path = 'data/kitti/integration/resized/training/labels/'\n",
    "imlabel_list = helper.imlabel(impath, labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alert-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/kitti/integration/resized/training/images/000000_resized.jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imlabel_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-ancient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Truck</td>\n",
       "      <td>[869.5441066666667, 120.88888888888889, 913.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Cyclist</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  Pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "1  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...       Truck  [869.5441066666667, 120.88888888888889, 913.55...\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         Car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...     Cyclist  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [730.9764266666667, 131.17681159420292, 856.77..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['image_path','label_path','class','bboxes'])\n",
    "# df = pd.DataFrame()\n",
    "# df['image_path'] = imlabel_list[:][0]\n",
    "# df['label_path'] = imlabel_list[:][1]\n",
    "# df.head()\n",
    "\n",
    "for item in imlabel_list:\n",
    "    #bboxes = helper.fetch_bboxes(item[1]).tolist()\n",
    "    bboxes = helper.fetch_bboxes(item[1])\n",
    "    classes = helper.fetch_classes(item[1])\n",
    "    for i,cls_bbox in enumerate(zip(classes,bboxes)):\n",
    "        # print(cls_bbox)\n",
    "        #df.loc[i,'image_path'] = item[0]\n",
    "        #df.loc[i,'label_path'] = item[1]\n",
    "        #df.loc[i,'class'] = cls_bbox[0]\n",
    "        \n",
    "        #df.loc[i,'bboxes'] = cls_bbox[1]\n",
    "        # df = df.append([item[0],item[1],cls_bbox[0],cls_bbox[1]])\n",
    "        df = df.append({'image_path':item[0], 'label_path': item[1],\n",
    "                        'class': cls_bbox[0], 'bboxes': cls_bbox[1][:4]}, ignore_index=True)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-society",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Car               42505\n",
       "DontCare          17015\n",
       "Pedestrian         6686\n",
       "Van                4283\n",
       "Cyclist            2424\n",
       "Truck              1614\n",
       "Misc               1394\n",
       "Tram                743\n",
       "Person_sitting      304\n",
       "2-wheeler           122\n",
       "pedestrian           70\n",
       "car                  50\n",
       "dontcare              1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "commercial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes that we don't need\n",
    "\n",
    "remove_classes = ['Truck', 'Misc', 'Tram']\n",
    "\n",
    "for item in remove_classes:\n",
    "    df.drop(df[df['class'] == item].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bright-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Cyclist</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  Pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         Car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...     Cyclist  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlikely-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>2-wheeler</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>dontcare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>dontcare</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...   2-wheeler  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    dontcare  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    dontcare  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clubbing similar categories\n",
    "class_dict = {'Car': 'car','car':'car', \n",
    "              'Pedestrian': 'pedestrian','pedestrian': 'pedestrian',\n",
    "              '2-wheeler':'2-wheeler', 'Van': 'car', 'dontcare': 'dontcare', \n",
    "              'Cyclist': '2-wheeler', 'Person_sitting' : 'pedestrian', 'DontCare' : 'dontcare'}\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x:  class_dict[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compatible-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car           46838\n",
       "dontcare      17016\n",
       "pedestrian     7060\n",
       "2-wheeler      2546\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bright-racing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>2</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>0</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>3</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>3</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path  class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      1  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      2  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      0  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      3  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      3  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'2-wheeler': 0, \n",
    "              'pedestrian': 1, 'car': 2,\n",
    "              'dontcare': 3}\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x:  class_dict[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "warming-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Split\n",
    "X = df.image_path\n",
    "y = df[['bboxes', 'class']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imported-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im_arr):\n",
    "    # Normalizes image with imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im_arr - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "class KittiDS(Dataset):\n",
    "    def __init__(self, paths, bboxes, y):\n",
    "        # self.transforms = transforms\n",
    "        self.paths = paths.values\n",
    "        self.bboxes = bboxes.values\n",
    "        self.y = y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        y_class = self.y[idx]\n",
    "        y_bbox = self.bboxes[idx]\n",
    "        # x, y_bb = transformsXY(path, self.bb[idx], self.transforms)\n",
    "        x = cv2.cvtColor(cv2.imread(path).astype('float32'),\n",
    "                         cv2.COLOR_BGR2RGB)/255\n",
    "        x = normalize(x)\n",
    "        x = np.rollaxis(x, 2)\n",
    "        return x, y_bbox, y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ethical-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_kitti = KittiDS(X_train, y_train['bboxes'], y_train['class'])\n",
    "#val_kitti = KittiDS(X_val, y_val['bboxes'], y_val['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beginning-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size\n",
    "#batch_size = 16\n",
    "# For autobatching and parallelizing data-loading\n",
    "#train_kitti_pt = DataLoader(train_kitti, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#val_kitti_pt = DataLoader(val_kitti, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elect-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flexing my GPU ^_^ :  GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# verify if GPU is being used with its name\n",
    "print(\"Flexing my GPU ^_^ : \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deluxe-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "french-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "practical-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyKitti_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyKitti_model, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # children returns immediate child modules\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        # classification network\n",
    "        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        # bbox regressor network\n",
    "        self.bbox = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        # print(\"x shape after extracting features1: \", x.shape)\n",
    "        x = self.features2(x)\n",
    "        # print(\"x shape after extracting features2: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        # print(\"x shape before reshape: \", x.shape)\n",
    "        # reshape tensor\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print(\"x shape: \", x.shape)\n",
    "        return self.classifier(x), self.bbox(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fiscal-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(model, optimizer, train_kitti_pt, val_kitti_pt, epochs=10,C=1000):\n",
    "    for i in range(epochs):\n",
    "        # enables model training, grad compute\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        correct = 0\n",
    "        for x, y_bbox, y_class in train_kitti_pt:\n",
    "            batch = y_class.shape[0]\n",
    "            # fp32 precision\n",
    "            # x = x.cuda().float()\n",
    "            # x = x.float().to(device, dtype=float)\n",
    "            x = x.float().to(device)\n",
    "            # x = x.to(device, dtype=torch.float16)\n",
    "            # print(\"x shape: \", x.shape)\n",
    "            # y_class = y_class.cuda()\n",
    "            y_class = y_class.long().to(device)\n",
    "            # print(\"y_class: \", y_class, 'y_class shape: ', y_class.shape)\n",
    "            # fp32 precision \n",
    "            # y_bbox = y_bbox.float().to(device, dtype=float)\n",
    "            y_bbox = y_bbox.float().to(device)\n",
    "            # y_bbox = y_bbox.to(device, dtype=torch.float16)\n",
    "            # print(\"y_bbox: \", y_bbox, \"\\ny_bbox shape\", y_bbox.shape)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            # print(\"out_bbox: \", out_bbox, \"\\nout_bbox shape\", out_bbox.shape)\n",
    "            # print(\"out_class: \", out_class, '\\nout_class shape: ', out_class.shape)\n",
    "            # compute classification loss: torch.max(outputs, 1)[1], torch.max(labels, 1)[1]\n",
    "            clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "            # compute L1 loss\n",
    "            bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "            # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # set gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "        train_loss = sum_loss/total\n",
    "        train_acc = correct/total\n",
    "        val_loss, val_acc = val_metrics(model, val_kitti_pt, C)\n",
    "        print(\"Epoch: \",i+1,\"/\",epochs,\"\\n----------------------------\")\n",
    "        print(\"Train_loss: %.3f, Train_acc: %.3f,\\nVal_loss: %.3f, Val_acc: %.3f\" % \n",
    "              (train_loss, train_acc,val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rough-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute validation metrics\n",
    "def val_metrics(model, val_kitti_pt, C=1000):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    # pair .eval() with no_grad\n",
    "    # turn off grad computation\n",
    "    with torch.no_grad():\n",
    "        for x, y_bbox, y_class in val_kitti_pt:\n",
    "            batch = y_class.shape[0]\n",
    "            # x = x.float().to(device, dtype=float)\n",
    "            x = x.float().to(device)\n",
    "            y_class = y_class.long().to(device)\n",
    "            y_bbox = y_bbox.float().to(device)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "            bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "attractive-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "PyKitti_model                                 --                        --\n",
       "├─Sequential: 1-1                             [16, 128, 68, 120]        --\n",
       "│    └─Conv2d: 2-1                            [16, 64, 272, 480]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [16, 64, 272, 480]        128\n",
       "│    └─ReLU: 2-3                              [16, 64, 272, 480]        --\n",
       "│    └─MaxPool2d: 2-4                         [16, 64, 136, 240]        --\n",
       "│    └─Sequential: 2-5                        [16, 64, 136, 240]        --\n",
       "│    │    └─BasicBlock: 3-1                   [16, 64, 136, 240]        73,984\n",
       "│    │    └─BasicBlock: 3-2                   [16, 64, 136, 240]        73,984\n",
       "│    │    └─BasicBlock: 3-3                   [16, 64, 136, 240]        73,984\n",
       "│    └─Sequential: 2-6                        [16, 128, 68, 120]        --\n",
       "│    │    └─BasicBlock: 3-4                   [16, 128, 68, 120]        230,144\n",
       "│    │    └─BasicBlock: 3-5                   [16, 128, 68, 120]        295,424\n",
       "│    │    └─BasicBlock: 3-6                   [16, 128, 68, 120]        295,424\n",
       "│    │    └─BasicBlock: 3-7                   [16, 128, 68, 120]        295,424\n",
       "├─Sequential: 1-2                             [16, 512, 17, 30]         --\n",
       "│    └─Sequential: 2-7                        [16, 256, 34, 60]         --\n",
       "│    │    └─BasicBlock: 3-8                   [16, 256, 34, 60]         919,040\n",
       "│    │    └─BasicBlock: 3-9                   [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-10                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-11                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-12                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-13                  [16, 256, 34, 60]         1,180,672\n",
       "│    └─Sequential: 2-8                        [16, 512, 17, 30]         --\n",
       "│    │    └─BasicBlock: 3-14                  [16, 512, 17, 30]         3,673,088\n",
       "│    │    └─BasicBlock: 3-15                  [16, 512, 17, 30]         4,720,640\n",
       "│    │    └─BasicBlock: 3-16                  [16, 512, 17, 30]         4,720,640\n",
       "├─Sequential: 1-3                             [16, 4]                   --\n",
       "│    └─BatchNorm1d: 2-9                       [16, 512]                 1,024\n",
       "│    └─Linear: 2-10                           [16, 4]                   2,052\n",
       "├─Sequential: 1-4                             [16, 4]                   --\n",
       "│    └─BatchNorm1d: 2-11                      [16, 512]                 1,024\n",
       "│    └─Linear: 2-12                           [16, 4]                   2,052\n",
       "===============================================================================================\n",
       "Total params: 21,290,824\n",
       "Trainable params: 21,290,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 610.04\n",
       "===============================================================================================\n",
       "Input size (MB): 100.27\n",
       "Forward/backward pass size (MB): 9960.29\n",
       "Params size (MB): 85.16\n",
       "Estimated Total Size (MB): 10145.73\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = PyKitti_model().to(device, dtype=torch.float32)\n",
    "# model1 = PyKitti_model().to(device)\n",
    "parameters = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.006)\n",
    "summary(model1, input_size=(16, 3, 544, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "immediate-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train(model1, optimizer, train_kitti_pt, val_kitti_pt, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-trade",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "australian-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Split\n",
    "X = df.image_path\n",
    "y = df[['bboxes', 'class']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the train, val dataset with the custom Dataset class\n",
    "train_kitti_amp = KittiDS(X_train, y_train['bboxes'], y_train['class'])\n",
    "val_kitti_amp = KittiDS(X_val, y_val['bboxes'], y_val['class'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Leverage torch dataloader for autobatching, parallelization\n",
    "train_kitti_amp = DataLoader(train_kitti_amp,\n",
    "        batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "val_kitti_amp = DataLoader(val_kitti_amp, \n",
    "                           batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer,best,epoch,loss,val_loss,path):\n",
    "    # Additional information\n",
    "    EPOCH = epoch\n",
    "    PATH = path\n",
    "    LOSS = loss\n",
    "\n",
    "    torch.save({\n",
    "                'epoch': EPOCH,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': LOSS,\n",
    "                'val_loss_min': val_loss\n",
    "                }, PATH)\n",
    "        \n",
    "    if best:\n",
    "        best_fpath = 'model/best_model/'\n",
    "        if !os.path.isdir(best_fpath):\n",
    "            os.mkdir(best_fpath,0o666)\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(path, best_fpath + os.path.basename(path))\n",
    "    \n",
    "def load_model():\n",
    "    model = model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hindu-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Mixed Precision\n",
    "def train_amp(model, optimizer, val_loss_min = np.inf, train_kitti_amp, val_kitti_amp, start_epoch=0,epochs=10, C=1000):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "\n",
    "    # X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    # y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, 0.006,\n",
    "        cycle_momentum=False,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=int(np.ceil(len(X) / batch_size)),\n",
    "    )\n",
    "    \n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # set the init scale of gradscaler to 2^14 instead of ^16\n",
    "    scaler = torch.cuda.amp.GradScaler(init_scale = 16384)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        for i, (x, y_bbox, y_class) in enumerate(train_kitti_amp):\n",
    "            batch = y_class.shape[0]\n",
    "            x = x.float().to(device)\n",
    "            y_bbox = y_bbox.float().to(device)\n",
    "            y_class = y_class.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # NEW\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #y_pred = model(X_batch).squeeze()\n",
    "                out_class, out_bbox = model(x)\n",
    "                clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "                # compute L1 loss\n",
    "                bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "                # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # NEW\n",
    "            scaler.scale(loss).backward()\n",
    "            total += batch\n",
    "            lv = loss.detach().cpu().numpy()\n",
    "            sum_loss += lv.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "\n",
    "            if i%100==0:\n",
    "                print(\"Epoch: %.3f, Batch: %d, Loss: %.3f\" % \n",
    "                      (i/len(train_kitti_amp),i,lv))\n",
    "\n",
    "            # NEW\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_loss = sum_loss/total\n",
    "        train_acc = correct/total\n",
    "        val_loss, val_acc = val_metrics_amp(model, val_kitti_amp, C)\n",
    "        # save model checkpoint\n",
    "        save_checkpoint(model,optimizer,best=False,\n",
    "                        epoch,lv,val_loss,\n",
    "                        path = 'model/model_amp_epoch_' + str(epoch))\n",
    "        \n",
    "        print(\"Epoch: \",i+1,\"/\",epochs,\"\\n----------------------------\")\n",
    "        print(\"Train_loss: %.3f, Train_acc: %.3f,\\nVal_loss: %.3f, Val_acc: %.3f\" % \n",
    "              (train_loss, train_acc,val_loss, val_acc))\n",
    "        \n",
    "        # save best model\n",
    "        if val_loss <= val_loss_min:\n",
    "            save_checkpoint(model,optimizer,best=True,\n",
    "                epoch,lv,val_loss,\n",
    "                path = 'model/model_amp_epoch_' + str(epoch))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "metallic-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_amp\n",
    "def val_metrics_amp(model, val_kitti_amp, C=1000):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    # pair .eval() with no_grad\n",
    "    # turn off grad computation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y_bbox, y_class) in enumerate(val_kitti_amp):\n",
    "            #x = torch.tensor(x, dtype=torch.float32)\n",
    "            #y_bbox = torch.tensor(y_bbox, dtype=torch.float32)\n",
    "            #y_class = torch.tensor(y_class, dtype=torch.long)\n",
    "            x = x.float().to(device)\n",
    "            y_bbox = y_bbox.float().to(device)\n",
    "            y_class = y_class.long().to(device)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            # NEW\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #y_pred = model(X_batch).squeeze()\n",
    "                out_class, out_bbox = model(x)\n",
    "                clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "                # compute L1 loss\n",
    "                bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "                # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # NEW\n",
    "            total += batch\n",
    "            lv = loss.detach().cpu().numpy()\n",
    "            sum_loss += lv.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "governmental-matrix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.000, Batch: 0, Loss: 20.541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-1da084b23b68>\u001b[0m in \u001b[0;36mtrain_amp\u001b[0;34m(model, optimizer, train_kitti_amp, val_kitti_amp, epochs, C)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_kitti_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-424887a185c5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# x, y_bb = transformsXY(path, self.bb[idx], self.transforms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         x = cv2.cvtColor(cv2.imread(path).astype('float32'),\n\u001b[0m\u001b[1;32m     22\u001b[0m                          cv2.COLOR_BGR2RGB)/255\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_amp(model1, optimizer, train_kitti_amp, val_kitti_amp, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-colon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
