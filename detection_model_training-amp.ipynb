{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "architectural-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from utils import helper\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supported-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "# Columns: image_path, label_path, bboxes, classes\n",
    "impath = 'data/kitti/integration/resized/training/images/'\n",
    "labels_path = 'data/kitti/integration/resized/training/labels/'\n",
    "imlabel_list = helper.imlabel(impath, labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alert-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/kitti/integration/resized/training/images/000000_resized.jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imlabel_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-ancient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Truck</td>\n",
       "      <td>[869.5441066666667, 120.88888888888889, 913.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Cyclist</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  Pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "1  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...       Truck  [869.5441066666667, 120.88888888888889, 913.55...\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         Car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...     Cyclist  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [730.9764266666667, 131.17681159420292, 856.77..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['image_path','label_path','class','bboxes'])\n",
    "# df = pd.DataFrame()\n",
    "# df['image_path'] = imlabel_list[:][0]\n",
    "# df['label_path'] = imlabel_list[:][1]\n",
    "# df.head()\n",
    "\n",
    "for item in imlabel_list:\n",
    "    #bboxes = helper.fetch_bboxes(item[1]).tolist()\n",
    "    bboxes = helper.fetch_bboxes(item[1])\n",
    "    classes = helper.fetch_classes(item[1])\n",
    "    for i,cls_bbox in enumerate(zip(classes,bboxes)):\n",
    "        # print(cls_bbox)\n",
    "        #df.loc[i,'image_path'] = item[0]\n",
    "        #df.loc[i,'label_path'] = item[1]\n",
    "        #df.loc[i,'class'] = cls_bbox[0]\n",
    "        \n",
    "        #df.loc[i,'bboxes'] = cls_bbox[1]\n",
    "        # df = df.append([item[0],item[1],cls_bbox[0],cls_bbox[1]])\n",
    "        df = df.append({'image_path':item[0], 'label_path': item[1],\n",
    "                        'class': cls_bbox[0], 'bboxes': cls_bbox[1][:4]}, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-society",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Car               42505\n",
       "DontCare          17015\n",
       "Pedestrian         6686\n",
       "Van                4283\n",
       "Cyclist            2424\n",
       "Truck              1614\n",
       "Misc               1394\n",
       "Tram                743\n",
       "Person_sitting      304\n",
       "2-wheeler           122\n",
       "pedestrian           70\n",
       "car                  50\n",
       "dontcare              1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "commercial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes that we don't need\n",
    "\n",
    "remove_classes = ['Truck', 'Misc', 'Tram']\n",
    "\n",
    "for item in remove_classes:\n",
    "    df.drop(df[df['class'] == item].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bright-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>Cyclist</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  Pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         Car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...     Cyclist  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    DontCare  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlikely-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>car</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>2-wheeler</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>dontcare</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>dontcare</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path       class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...  pedestrian  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...         car  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...   2-wheeler  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    dontcare  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...    dontcare  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clubbing similar categories\n",
    "class_dict = {'Car': 'car','car':'car', \n",
    "              'Pedestrian': 'pedestrian','pedestrian': 'pedestrian',\n",
    "              '2-wheeler':'2-wheeler', 'Van': 'car', 'dontcare': 'dontcare', \n",
    "              'Cyclist': '2-wheeler', 'Person_sitting' : 'pedestrian', 'DontCare' : 'dontcare'}\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x:  class_dict[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compatible-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car           46838\n",
       "dontcare      17016\n",
       "pedestrian     7060\n",
       "2-wheeler      2546\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bright-racing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>class</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1047.4205405405405, 112.15686274509804, 1191....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>2</td>\n",
       "      <td>[562.32192, 140.3207729468599, 614.80704, 157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>0</td>\n",
       "      <td>[981.5210666666668, 126.72463768115941, 999.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>3</td>\n",
       "      <td>[730.9764266666667, 131.17681159420292, 856.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/kitti/integration/resized/training/images...</td>\n",
       "      <td>data/kitti/integration/resized/training/labels...</td>\n",
       "      <td>3</td>\n",
       "      <td>[741.7984000000001, 135.23478260869567, 765.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path                                         label_path  class                                             bboxes\n",
       "0  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      1  [1047.4205405405405, 112.15686274509804, 1191....\n",
       "2  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      2  [562.32192, 140.3207729468599, 614.80704, 157....\n",
       "3  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      0  [981.5210666666668, 126.72463768115941, 999.48...\n",
       "4  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      3  [730.9764266666667, 131.17681159420292, 856.77...\n",
       "5  data/kitti/integration/resized/training/images...  data/kitti/integration/resized/training/labels...      3  [741.7984000000001, 135.23478260869567, 765.67..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'2-wheeler': 0, \n",
    "              'pedestrian': 1, 'car': 2,\n",
    "              'dontcare': 3}\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x:  class_dict[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "warming-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Split\n",
    "X = df.image_path\n",
    "y = df[['bboxes', 'class']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imported-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im_arr):\n",
    "    # Normalizes image with imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im_arr - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "class KittiDS(Dataset):\n",
    "    def __init__(self, paths, bboxes, y):\n",
    "        # self.transforms = transforms\n",
    "        self.paths = paths.values\n",
    "        self.bboxes = bboxes.values\n",
    "        self.y = y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        y_class = self.y[idx]\n",
    "        y_bbox = self.bboxes[idx]\n",
    "        # x, y_bb = transformsXY(path, self.bb[idx], self.transforms)\n",
    "        x = cv2.cvtColor(cv2.imread(path).astype('float32'),\n",
    "                         cv2.COLOR_BGR2RGB)/255\n",
    "        x = normalize(x)\n",
    "        x = np.rollaxis(x, 2)\n",
    "        return x, y_bbox, y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ethical-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_kitti = KittiDS(X_train, y_train['bboxes'], y_train['class'])\n",
    "#val_kitti = KittiDS(X_val, y_val['bboxes'], y_val['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beginning-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size\n",
    "#batch_size = 16\n",
    "# For autobatching and parallelizing data-loading\n",
    "#train_kitti_pt = DataLoader(train_kitti, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#val_kitti_pt = DataLoader(val_kitti, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elect-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flexing my GPU ^_^ :  GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# verify if GPU is being used with its name\n",
    "print(\"Flexing my GPU ^_^ : \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deluxe-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "french-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "practical-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyKitti_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyKitti_model, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # children returns immediate child modules\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        # classification network\n",
    "        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        # bbox regressor network\n",
    "        self.bbox = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        # print(\"x shape after extracting features1: \", x.shape)\n",
    "        x = self.features2(x)\n",
    "        # print(\"x shape after extracting features2: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        # print(\"x shape before reshape: \", x.shape)\n",
    "        # reshape tensor\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print(\"x shape: \", x.shape)\n",
    "        return self.classifier(x), self.bbox(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fiscal-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(model, optimizer, train_kitti_pt, val_kitti_pt, epochs=10,C=1000):\n",
    "    for i in range(epochs):\n",
    "        # enables model training, grad compute\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        correct = 0\n",
    "        for x, y_bbox, y_class in train_kitti_pt:\n",
    "            batch = y_class.shape[0]\n",
    "            # fp32 precision\n",
    "            # x = x.cuda().float()\n",
    "            # x = x.float().to(device, dtype=float)\n",
    "            x = x.float().to(device, non_blocking=True)\n",
    "            # x = x.to(device, dtype=torch.float16)\n",
    "            # print(\"x shape: \", x.shape)\n",
    "            # y_class = y_class.cuda()\n",
    "            y_class = y_class.long().to(device, non_blocking=True)\n",
    "            # print(\"y_class: \", y_class, 'y_class shape: ', y_class.shape)\n",
    "            # fp32 precision \n",
    "            # y_bbox = y_bbox.float().to(device, dtype=float)\n",
    "            y_bbox = y_bbox.float().to(device, non_blocking=True)\n",
    "            # y_bbox = y_bbox.to(device, dtype=torch.float16)\n",
    "            # print(\"y_bbox: \", y_bbox, \"\\ny_bbox shape\", y_bbox.shape)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            # print(\"out_bbox: \", out_bbox, \"\\nout_bbox shape\", out_bbox.shape)\n",
    "            # print(\"out_class: \", out_class, '\\nout_class shape: ', out_class.shape)\n",
    "            # compute classification loss: torch.max(outputs, 1)[1], torch.max(labels, 1)[1]\n",
    "            clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "            # compute L1 loss\n",
    "            bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "            # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # set gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "        train_loss = sum_loss/total\n",
    "        train_acc = correct/total\n",
    "        val_loss, val_acc = val_metrics(model, val_kitti_pt, C)\n",
    "        print(\"Epoch: \",i+1,\"/\",epochs,\"\\n----------------------------\")\n",
    "        print(\"Train_loss: %.3f, Train_acc: %.3f,\\nVal_loss: %.3f, Val_acc: %.3f\" % \n",
    "              (train_loss, train_acc,val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rough-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute validation metrics\n",
    "def val_metrics(model, val_kitti_pt, C=1000):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    # pair .eval() with no_grad\n",
    "    # turn off grad computation\n",
    "    with torch.no_grad():\n",
    "        for x, y_bbox, y_class in val_kitti_pt:\n",
    "            batch = y_class.shape[0]\n",
    "            # x = x.float().to(device, dtype=float)\n",
    "            x = x.float().to(device, non_blocking=True)\n",
    "            y_class = y_class.long().to(device, non_blocking=True)\n",
    "            y_bbox = y_bbox.float().to(device, non_blocking=True)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "            bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = PyKitti_model().to(device, dtype=torch.float32)\n",
    "# model1 = PyKitti_model().to(device)\n",
    "parameters = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.006)\n",
    "summary(model1, input_size=(16, 3, 544, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train(model1, optimizer, train_kitti_pt, val_kitti_pt, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-trade",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "australian-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train, val dataset with the custom Dataset class\n",
    "train_kitti_amp = KittiDS(X_train, y_train['bboxes'], y_train['class'])\n",
    "val_kitti_amp = KittiDS(X_val, y_val['bboxes'], y_val['class'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Leverage torch dataloader for autobatching, parallelization\n",
    "train_kitti_amp = DataLoader(train_kitti_amp,\n",
    "        batch_size=batch_size, num_workers=12, shuffle=True, drop_last=True\n",
    "    )\n",
    "val_kitti_amp = DataLoader(val_kitti_amp, \n",
    "                           batch_size=batch_size, num_workers=12, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "banned-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model,optimizer,best,epoch,loss,val_loss,path):\n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'val_loss_min': val_loss\n",
    "                }, path)\n",
    "        \n",
    "    if best:\n",
    "        best_fpath = 'model/best_model/'\n",
    "        if not(os.path.isdir(best_fpath)):\n",
    "            os.mkdir(best_fpath,0o666)\n",
    "        # copy checkpoint file to best model path folder\n",
    "        shutil.copyfile(path, best_fpath + os.path.basename(path))\n",
    "    \n",
    "def load_model(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    val_loss = checkpoint['val_loss_min']\n",
    "    return (model, optimizer, epoch, loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hindu-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Mixed Precision\n",
    "def train_amp(model, optimizer, train_kitti_amp, val_kitti_amp, \n",
    "              val_loss_min = np.inf, start_epoch=0,epochs=10, C=1000):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "\n",
    "    # X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    # y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, 0.0005,\n",
    "        cycle_momentum=False,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=int(np.ceil(len(X) / batch_size)),\n",
    "    )\n",
    "    \n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # set the init scale of gradscaler to 2^14 instead of ^16\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        scheduler.step()\n",
    "        for i, (x, y_bbox, y_class) in enumerate(train_kitti_amp):\n",
    "            batch = y_class.shape[0]\n",
    "            x = x.float().to(device, non_blocking=True)\n",
    "            y_bbox = y_bbox.float().to(device, non_blocking=True)\n",
    "            y_class = y_class.long().to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # NEW\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #y_pred = model(X_batch).squeeze()\n",
    "                out_class, out_bbox = model(x)\n",
    "                clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "                # compute L1 loss\n",
    "                bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "            # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # NEW\n",
    "            scaler.scale(loss).backward()\n",
    "            total += batch\n",
    "            lv = loss.detach().cpu().numpy()\n",
    "            sum_loss += lv.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "\n",
    "            if i%100==0:\n",
    "                print(\"Epoch: %.3f, Batch: %d, Loss: %.3f\" % \n",
    "                      (epoch + (i/len(train_kitti_amp)),i,lv))\n",
    "\n",
    "            # NEW\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        train_loss = sum_loss/total\n",
    "        train_acc = correct/total\n",
    "        val_loss, val_acc = val_metrics_amp(model, val_kitti_amp, C)\n",
    "        # save model checkpoint\n",
    "        save_checkpoint(model,optimizer,False,\n",
    "                        epoch,train_loss,val_loss,\n",
    "                        path = 'model/model_amp_test2_epoch_' + str(epoch))\n",
    "        \n",
    "        print(\"Epoch: \",epoch+1,\"/\",epochs,\"\\n----------------------------\")\n",
    "        print(\"Train_loss: %.3f, Train_acc: %.3f,\\nVal_loss: %.3f, Val_acc: %.3f\" % \n",
    "              (train_loss, train_acc,val_loss, val_acc))\n",
    "        \n",
    "        # save best model\n",
    "        if val_loss <= val_loss_min:\n",
    "            val_loss_min = val_loss\n",
    "            save_checkpoint(model,optimizer,True,\n",
    "                epoch,train_loss,val_loss,\n",
    "                path = 'model/model_amp_epoch_' + str(epoch))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metallic-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_amp\n",
    "def val_metrics_amp(model, val_kitti_amp, C=1000):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    # pair .eval() with no_grad\n",
    "    # turn off grad computation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y_bbox, y_class) in enumerate(val_kitti_amp):\n",
    "            #x = torch.tensor(x, dtype=torch.float32)\n",
    "            #y_bbox = torch.tensor(y_bbox, dtype=torch.float32)\n",
    "            #y_class = torch.tensor(y_class, dtype=torch.long)\n",
    "            batch = y_class.shape[0]\n",
    "            x = x.float().to(device, non_blocking=True)\n",
    "            y_bbox = y_bbox.float().to(device, non_blocking=True)\n",
    "            y_class = y_class.long().to(device, non_blocking=True)\n",
    "            out_class, out_bbox = model.forward(x)\n",
    "            # NEW\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #y_pred = model(X_batch).squeeze()\n",
    "                out_class, out_bbox = model(x)\n",
    "                clf_loss = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "                # compute L1 loss\n",
    "                bbox_reg_loss = F.l1_loss(out_bbox, y_bbox, reduction=\"none\").sum(1)\n",
    "            bbox_reg_loss = bbox_reg_loss.sum()\n",
    "                # computing total loss\n",
    "            loss = clf_loss + bbox_reg_loss/C\n",
    "            # NEW\n",
    "            lv = loss.detach().cpu().numpy()\n",
    "            sum_loss += lv.item()\n",
    "            _, pred = torch.max(out_class, 1)\n",
    "            correct += pred.eq(y_class).sum().item()\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_amp(model1, optimizer, train_kitti_amp, val_kitti_amp, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "innovative-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 19 07:06:37 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3080    Off  | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   41C    P5    28W / 320W |    454MiB / 10014MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications for Model-2\n",
    "class PyKitti_model_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyKitti_model_test, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # children returns immediate child modules\n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features1 = nn.Sequential(*layers[:4])\n",
    "        self.features2 = nn.Sequential(*layers[4:])\n",
    "        # classification network\n",
    "        self.classifier = nn.Sequential(nn.BatchNorm1d(128), nn.Linear(128, 4))\n",
    "        # bbox regressor network\n",
    "        self.bbox = nn.Sequential(nn.BatchNorm1d(128), nn.Linear(128, 4))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        # print(\"x shape after extracting features1: \", x.shape)\n",
    "        x = self.features2(x)\n",
    "        # print(\"x shape after extracting features2: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        # print(\"x shape before reshape: \", x.shape)\n",
    "        # reshape tensor\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print(\"x shape: \", x.shape)\n",
    "        return self.classifier(x), self.bbox(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = PyKitti_model_test().to(device, dtype=torch.float32)\n",
    "# model1 = PyKitti_model().to(device)\n",
    "parameters = filter(lambda p: p.requires_grad, model2.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005)\n",
    "summary(model2, input_size=(16, 3, 544, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU-GPU transfer augmented before pin_memory and non-blocking \n",
    "%%time\n",
    "train_amp(model2, optimizer, train_kitti_amp, val_kitti_amp, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# after pin_memory and non-blocking\n",
    "train_amp(model2, optimizer, train_kitti_amp, val_kitti_amp, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-magnitude",
   "metadata": {},
   "source": [
    "It was not useful XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tropical-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications for Model-2\n",
    "class PyKitti_model_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyKitti_model_v2, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # children returns immediate child modules\n",
    "        layers = list(resnet.children())[:9]\n",
    "        self.features1 = nn.Sequential(*layers[:7])\n",
    "        self.features2 = nn.Sequential(*layers[7:])\n",
    "        # classification network\n",
    "        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        # bbox regressor network\n",
    "        self.bbox = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        # print(\"x shape after extracting features1: \", x.shape)\n",
    "        x = self.features2(x)\n",
    "        # print(\"x shape after extracting features2: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        # print(\"x shape before reshape: \", x.shape)\n",
    "        # reshape tensor\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print(\"x shape: \", x.shape)\n",
    "        return self.classifier(x), self.bbox(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stupid-celebrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "PyKitti_model_v2                              --                        --\n",
       "├─Sequential: 1-1                             [16, 256, 34, 60]         --\n",
       "│    └─Conv2d: 2-1                            [16, 64, 272, 480]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [16, 64, 272, 480]        128\n",
       "│    └─ReLU: 2-3                              [16, 64, 272, 480]        --\n",
       "│    └─MaxPool2d: 2-4                         [16, 64, 136, 240]        --\n",
       "│    └─Sequential: 2-5                        [16, 64, 136, 240]        --\n",
       "│    │    └─BasicBlock: 3-1                   [16, 64, 136, 240]        73,984\n",
       "│    │    └─BasicBlock: 3-2                   [16, 64, 136, 240]        73,984\n",
       "│    │    └─BasicBlock: 3-3                   [16, 64, 136, 240]        73,984\n",
       "│    └─Sequential: 2-6                        [16, 128, 68, 120]        --\n",
       "│    │    └─BasicBlock: 3-4                   [16, 128, 68, 120]        230,144\n",
       "│    │    └─BasicBlock: 3-5                   [16, 128, 68, 120]        295,424\n",
       "│    │    └─BasicBlock: 3-6                   [16, 128, 68, 120]        295,424\n",
       "│    │    └─BasicBlock: 3-7                   [16, 128, 68, 120]        295,424\n",
       "│    └─Sequential: 2-7                        [16, 256, 34, 60]         --\n",
       "│    │    └─BasicBlock: 3-8                   [16, 256, 34, 60]         919,040\n",
       "│    │    └─BasicBlock: 3-9                   [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-10                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-11                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-12                  [16, 256, 34, 60]         1,180,672\n",
       "│    │    └─BasicBlock: 3-13                  [16, 256, 34, 60]         1,180,672\n",
       "├─Sequential: 1-2                             [16, 512, 1, 1]           --\n",
       "│    └─Sequential: 2-8                        [16, 512, 17, 30]         --\n",
       "│    │    └─BasicBlock: 3-14                  [16, 512, 17, 30]         3,673,088\n",
       "│    │    └─BasicBlock: 3-15                  [16, 512, 17, 30]         4,720,640\n",
       "│    │    └─BasicBlock: 3-16                  [16, 512, 17, 30]         4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [16, 512, 1, 1]           --\n",
       "├─Sequential: 1-3                             [16, 4]                   --\n",
       "│    └─BatchNorm1d: 2-10                      [16, 512]                 1,024\n",
       "│    └─Linear: 2-11                           [16, 4]                   2,052\n",
       "├─Sequential: 1-4                             [16, 4]                   --\n",
       "│    └─BatchNorm1d: 2-12                      [16, 512]                 1,024\n",
       "│    └─Linear: 2-13                           [16, 4]                   2,052\n",
       "===============================================================================================\n",
       "Total params: 21,290,824\n",
       "Trainable params: 21,290,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 610.04\n",
       "===============================================================================================\n",
       "Input size (MB): 100.27\n",
       "Forward/backward pass size (MB): 9960.29\n",
       "Params size (MB): 85.16\n",
       "Estimated Total Size (MB): 10145.73\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = PyKitti_model_v2().to(device, dtype=torch.float32)\n",
    "# model1 = PyKitti_model().to(device)\n",
    "parameters = filter(lambda p: p.requires_grad, model3.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.005, betas=(0.9, 0.999), eps=1e-08)\n",
    "summary(model3, input_size=(16, 3, 544, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "competitive-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.000, Batch: 0, Loss: 50.102\n",
      "Epoch: 0.027, Batch: 100, Loss: 55.524\n",
      "Epoch: 0.054, Batch: 200, Loss: 41.419\n",
      "Epoch: 0.082, Batch: 300, Loss: 60.394\n",
      "Epoch: 0.109, Batch: 400, Loss: 60.191\n",
      "Epoch: 0.136, Batch: 500, Loss: 39.531\n",
      "Epoch: 0.163, Batch: 600, Loss: 44.066\n",
      "Epoch: 0.191, Batch: 700, Loss: 38.937\n",
      "Epoch: 0.218, Batch: 800, Loss: 48.704\n",
      "Epoch: 0.245, Batch: 900, Loss: 40.246\n",
      "Epoch: 0.272, Batch: 1000, Loss: 33.831\n",
      "Epoch: 0.299, Batch: 1100, Loss: 37.887\n",
      "Epoch: 0.327, Batch: 1200, Loss: 49.573\n",
      "Epoch: 0.354, Batch: 1300, Loss: 39.438\n",
      "Epoch: 0.381, Batch: 1400, Loss: 38.069\n",
      "Epoch: 0.408, Batch: 1500, Loss: 54.008\n",
      "Epoch: 0.436, Batch: 1600, Loss: 47.018\n",
      "Epoch: 0.463, Batch: 1700, Loss: 45.020\n",
      "Epoch: 0.490, Batch: 1800, Loss: 37.732\n",
      "Epoch: 0.517, Batch: 1900, Loss: 42.032\n",
      "Epoch: 0.545, Batch: 2000, Loss: 47.588\n",
      "Epoch: 0.572, Batch: 2100, Loss: 47.946\n",
      "Epoch: 0.599, Batch: 2200, Loss: 38.658\n",
      "Epoch: 0.626, Batch: 2300, Loss: 36.113\n",
      "Epoch: 0.653, Batch: 2400, Loss: 48.905\n",
      "Epoch: 0.681, Batch: 2500, Loss: 46.900\n",
      "Epoch: 0.708, Batch: 2600, Loss: 36.828\n",
      "Epoch: 0.735, Batch: 2700, Loss: 49.901\n",
      "Epoch: 0.762, Batch: 2800, Loss: 36.117\n",
      "Epoch: 0.790, Batch: 2900, Loss: 46.060\n",
      "Epoch: 0.817, Batch: 3000, Loss: 41.594\n",
      "Epoch: 0.844, Batch: 3100, Loss: 44.825\n",
      "Epoch: 0.871, Batch: 3200, Loss: 45.121\n",
      "Epoch: 0.898, Batch: 3300, Loss: 42.279\n",
      "Epoch: 0.926, Batch: 3400, Loss: 43.978\n",
      "Epoch: 0.953, Batch: 3500, Loss: 38.771\n",
      "Epoch: 0.980, Batch: 3600, Loss: 51.269\n",
      "Epoch:  1 / 10 \n",
      "----------------------------\n",
      "Train_loss: 2.834, Train_acc: 0.674,\n",
      "Val_loss: 5.347, Val_acc: 0.713\n",
      "Epoch: 1.000, Batch: 0, Loss: 41.007\n",
      "Epoch: 1.027, Batch: 100, Loss: 37.941\n",
      "Epoch: 1.054, Batch: 200, Loss: 35.937\n",
      "Epoch: 1.082, Batch: 300, Loss: 43.407\n",
      "Epoch: 1.109, Batch: 400, Loss: 29.168\n",
      "Epoch: 1.136, Batch: 500, Loss: 35.860\n",
      "Epoch: 1.163, Batch: 600, Loss: 25.897\n",
      "Epoch: 1.191, Batch: 700, Loss: 30.769\n",
      "Epoch: 1.218, Batch: 800, Loss: 24.936\n",
      "Epoch: 1.245, Batch: 900, Loss: 28.322\n",
      "Epoch: 1.272, Batch: 1000, Loss: 30.006\n",
      "Epoch: 1.299, Batch: 1100, Loss: 30.094\n",
      "Epoch: 1.327, Batch: 1200, Loss: 24.170\n",
      "Epoch: 1.354, Batch: 1300, Loss: 22.330\n",
      "Epoch: 1.381, Batch: 1400, Loss: 29.088\n",
      "Epoch: 1.408, Batch: 1500, Loss: 33.891\n",
      "Epoch: 1.436, Batch: 1600, Loss: 25.552\n",
      "Epoch: 1.463, Batch: 1700, Loss: 17.352\n",
      "Epoch: 1.490, Batch: 1800, Loss: 23.372\n",
      "Epoch: 1.517, Batch: 1900, Loss: 18.334\n",
      "Epoch: 1.545, Batch: 2000, Loss: 16.519\n",
      "Epoch: 1.572, Batch: 2100, Loss: 31.483\n",
      "Epoch: 1.599, Batch: 2200, Loss: 26.232\n",
      "Epoch: 1.626, Batch: 2300, Loss: 18.247\n",
      "Epoch: 1.653, Batch: 2400, Loss: 22.739\n",
      "Epoch: 1.681, Batch: 2500, Loss: 24.046\n",
      "Epoch: 1.708, Batch: 2600, Loss: 25.222\n",
      "Epoch: 1.735, Batch: 2700, Loss: 24.869\n",
      "Epoch: 1.762, Batch: 2800, Loss: 20.742\n",
      "Epoch: 1.790, Batch: 2900, Loss: 24.255\n",
      "Epoch: 1.817, Batch: 3000, Loss: 24.404\n",
      "Epoch: 1.844, Batch: 3100, Loss: 28.817\n",
      "Epoch: 1.871, Batch: 3200, Loss: 24.559\n",
      "Epoch: 1.898, Batch: 3300, Loss: 16.831\n",
      "Epoch: 1.926, Batch: 3400, Loss: 22.204\n",
      "Epoch: 1.953, Batch: 3500, Loss: 17.948\n",
      "Epoch: 1.980, Batch: 3600, Loss: 21.849\n",
      "Epoch:  2 / 10 \n",
      "----------------------------\n",
      "Train_loss: 2.239, Train_acc: 0.686,\n",
      "Val_loss: 2.763, Val_acc: 0.709\n",
      "Epoch: 2.000, Batch: 0, Loss: 19.225\n",
      "Epoch: 2.027, Batch: 100, Loss: 27.878\n",
      "Epoch: 2.054, Batch: 200, Loss: 13.990\n",
      "Epoch: 2.082, Batch: 300, Loss: 16.703\n",
      "Epoch: 2.109, Batch: 400, Loss: 32.663\n",
      "Epoch: 2.136, Batch: 500, Loss: 23.037\n",
      "Epoch: 2.163, Batch: 600, Loss: 19.332\n",
      "Epoch: 2.191, Batch: 700, Loss: 24.988\n",
      "Epoch: 2.218, Batch: 800, Loss: 15.930\n",
      "Epoch: 2.245, Batch: 900, Loss: 22.830\n",
      "Epoch: 2.272, Batch: 1000, Loss: 16.562\n",
      "Epoch: 2.299, Batch: 1100, Loss: 18.646\n",
      "Epoch: 2.327, Batch: 1200, Loss: 22.855\n",
      "Epoch: 2.354, Batch: 1300, Loss: 18.370\n",
      "Epoch: 2.381, Batch: 1400, Loss: 23.776\n",
      "Epoch: 2.408, Batch: 1500, Loss: 24.806\n",
      "Epoch: 2.436, Batch: 1600, Loss: 23.713\n",
      "Epoch: 2.463, Batch: 1700, Loss: 22.871\n",
      "Epoch: 2.490, Batch: 1800, Loss: 22.223\n",
      "Epoch: 2.517, Batch: 1900, Loss: 13.218\n",
      "Epoch: 2.545, Batch: 2000, Loss: 25.009\n",
      "Epoch: 2.572, Batch: 2100, Loss: 24.011\n",
      "Epoch: 2.599, Batch: 2200, Loss: 25.078\n",
      "Epoch: 2.626, Batch: 2300, Loss: 21.873\n",
      "Epoch: 2.653, Batch: 2400, Loss: 12.945\n",
      "Epoch: 2.681, Batch: 2500, Loss: 19.880\n",
      "Epoch: 2.708, Batch: 2600, Loss: 30.425\n",
      "Epoch: 2.735, Batch: 2700, Loss: 20.808\n",
      "Epoch: 2.762, Batch: 2800, Loss: 19.831\n",
      "Epoch: 2.790, Batch: 2900, Loss: 19.679\n",
      "Epoch: 2.817, Batch: 3000, Loss: 20.479\n",
      "Epoch: 2.844, Batch: 3100, Loss: 32.159\n",
      "Epoch: 2.871, Batch: 3200, Loss: 15.305\n",
      "Epoch: 2.898, Batch: 3300, Loss: 21.690\n",
      "Epoch: 2.926, Batch: 3400, Loss: 29.516\n",
      "Epoch: 2.953, Batch: 3500, Loss: 24.374\n",
      "Epoch: 2.980, Batch: 3600, Loss: 26.045\n",
      "Epoch:  3 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.943, Train_acc: 0.693,\n",
      "Val_loss: 2.716, Val_acc: 0.686\n",
      "Epoch: 3.000, Batch: 0, Loss: 20.262\n",
      "Epoch: 3.027, Batch: 100, Loss: 23.419\n",
      "Epoch: 3.054, Batch: 200, Loss: 22.216\n",
      "Epoch: 3.082, Batch: 300, Loss: 22.434\n",
      "Epoch: 3.109, Batch: 400, Loss: 24.334\n",
      "Epoch: 3.136, Batch: 500, Loss: 20.253\n",
      "Epoch: 3.163, Batch: 600, Loss: 11.781\n",
      "Epoch: 3.191, Batch: 700, Loss: 18.533\n",
      "Epoch: 3.218, Batch: 800, Loss: 22.636\n",
      "Epoch: 3.245, Batch: 900, Loss: 19.814\n",
      "Epoch: 3.272, Batch: 1000, Loss: 22.957\n",
      "Epoch: 3.299, Batch: 1100, Loss: 25.070\n",
      "Epoch: 3.327, Batch: 1200, Loss: 16.306\n",
      "Epoch: 3.354, Batch: 1300, Loss: 15.468\n",
      "Epoch: 3.381, Batch: 1400, Loss: 26.893\n",
      "Epoch: 3.408, Batch: 1500, Loss: 19.509\n",
      "Epoch: 3.436, Batch: 1600, Loss: 25.377\n",
      "Epoch: 3.463, Batch: 1700, Loss: 16.972\n",
      "Epoch: 3.490, Batch: 1800, Loss: 27.020\n",
      "Epoch: 3.517, Batch: 1900, Loss: 21.733\n",
      "Epoch: 3.545, Batch: 2000, Loss: 22.159\n",
      "Epoch: 3.572, Batch: 2100, Loss: 26.367\n",
      "Epoch: 3.599, Batch: 2200, Loss: 25.054\n",
      "Epoch: 3.626, Batch: 2300, Loss: 20.546\n",
      "Epoch: 3.653, Batch: 2400, Loss: 23.536\n",
      "Epoch: 3.681, Batch: 2500, Loss: 25.107\n",
      "Epoch: 3.708, Batch: 2600, Loss: 20.266\n",
      "Epoch: 3.735, Batch: 2700, Loss: 20.266\n",
      "Epoch: 3.762, Batch: 2800, Loss: 27.223\n",
      "Epoch: 3.790, Batch: 2900, Loss: 17.843\n",
      "Epoch: 3.817, Batch: 3000, Loss: 15.198\n",
      "Epoch: 3.844, Batch: 3100, Loss: 16.884\n",
      "Epoch: 3.871, Batch: 3200, Loss: 19.335\n",
      "Epoch: 3.898, Batch: 3300, Loss: 24.598\n",
      "Epoch: 3.926, Batch: 3400, Loss: 19.189\n",
      "Epoch: 3.953, Batch: 3500, Loss: 17.445\n",
      "Epoch: 3.980, Batch: 3600, Loss: 16.359\n",
      "Epoch:  4 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.784, Train_acc: 0.697,\n",
      "Val_loss: 2.573, Val_acc: 0.715\n",
      "Epoch: 4.000, Batch: 0, Loss: 27.317\n",
      "Epoch: 4.027, Batch: 100, Loss: 21.420\n",
      "Epoch: 4.054, Batch: 200, Loss: 24.165\n",
      "Epoch: 4.082, Batch: 300, Loss: 26.081\n",
      "Epoch: 4.109, Batch: 400, Loss: 20.003\n",
      "Epoch: 4.136, Batch: 500, Loss: 19.370\n",
      "Epoch: 4.163, Batch: 600, Loss: 23.990\n",
      "Epoch: 4.191, Batch: 700, Loss: 23.090\n",
      "Epoch: 4.218, Batch: 800, Loss: 16.665\n",
      "Epoch: 4.245, Batch: 900, Loss: 15.849\n",
      "Epoch: 4.272, Batch: 1000, Loss: 17.598\n",
      "Epoch: 4.299, Batch: 1100, Loss: 21.397\n",
      "Epoch: 4.327, Batch: 1200, Loss: 21.765\n",
      "Epoch: 4.354, Batch: 1300, Loss: 23.677\n",
      "Epoch: 4.381, Batch: 1400, Loss: 11.696\n",
      "Epoch: 4.408, Batch: 1500, Loss: 21.079\n",
      "Epoch: 4.436, Batch: 1600, Loss: 22.136\n",
      "Epoch: 4.463, Batch: 1700, Loss: 28.229\n",
      "Epoch: 4.490, Batch: 1800, Loss: 23.391\n",
      "Epoch: 4.517, Batch: 1900, Loss: 21.009\n",
      "Epoch: 4.545, Batch: 2000, Loss: 20.469\n",
      "Epoch: 4.572, Batch: 2100, Loss: 24.193\n",
      "Epoch: 4.599, Batch: 2200, Loss: 18.705\n",
      "Epoch: 4.626, Batch: 2300, Loss: 19.594\n",
      "Epoch: 4.653, Batch: 2400, Loss: 25.553\n",
      "Epoch: 4.681, Batch: 2500, Loss: 20.408\n",
      "Epoch: 4.708, Batch: 2600, Loss: 16.362\n",
      "Epoch: 4.735, Batch: 2700, Loss: 20.837\n",
      "Epoch: 4.762, Batch: 2800, Loss: 20.226\n",
      "Epoch: 4.790, Batch: 2900, Loss: 26.033\n",
      "Epoch: 4.817, Batch: 3000, Loss: 22.141\n",
      "Epoch: 4.844, Batch: 3100, Loss: 21.219\n",
      "Epoch: 4.871, Batch: 3200, Loss: 18.035\n",
      "Epoch: 4.898, Batch: 3300, Loss: 17.301\n",
      "Epoch: 4.926, Batch: 3400, Loss: 16.830\n",
      "Epoch: 4.953, Batch: 3500, Loss: 23.433\n",
      "Epoch: 4.980, Batch: 3600, Loss: 23.973\n",
      "Epoch:  5 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.681, Train_acc: 0.700,\n",
      "Val_loss: 2.603, Val_acc: 0.712\n",
      "Epoch: 5.000, Batch: 0, Loss: 25.910\n",
      "Epoch: 5.027, Batch: 100, Loss: 20.569\n",
      "Epoch: 5.054, Batch: 200, Loss: 22.555\n",
      "Epoch: 5.082, Batch: 300, Loss: 17.771\n",
      "Epoch: 5.109, Batch: 400, Loss: 19.053\n",
      "Epoch: 5.136, Batch: 500, Loss: 25.068\n",
      "Epoch: 5.163, Batch: 600, Loss: 16.273\n",
      "Epoch: 5.191, Batch: 700, Loss: 26.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.218, Batch: 800, Loss: 18.517\n",
      "Epoch: 5.245, Batch: 900, Loss: 20.743\n",
      "Epoch: 5.272, Batch: 1000, Loss: 22.026\n",
      "Epoch: 5.299, Batch: 1100, Loss: 21.255\n",
      "Epoch: 5.327, Batch: 1200, Loss: 21.188\n",
      "Epoch: 5.354, Batch: 1300, Loss: 20.173\n",
      "Epoch: 5.381, Batch: 1400, Loss: 26.553\n",
      "Epoch: 5.408, Batch: 1500, Loss: 22.781\n",
      "Epoch: 5.436, Batch: 1600, Loss: 19.616\n",
      "Epoch: 5.463, Batch: 1700, Loss: 18.186\n",
      "Epoch: 5.490, Batch: 1800, Loss: 24.614\n",
      "Epoch: 5.517, Batch: 1900, Loss: 14.135\n",
      "Epoch: 5.545, Batch: 2000, Loss: 21.486\n",
      "Epoch: 5.572, Batch: 2100, Loss: 14.631\n",
      "Epoch: 5.599, Batch: 2200, Loss: 17.326\n",
      "Epoch: 5.626, Batch: 2300, Loss: 26.886\n",
      "Epoch: 5.653, Batch: 2400, Loss: 19.021\n",
      "Epoch: 5.681, Batch: 2500, Loss: 16.196\n",
      "Epoch: 5.708, Batch: 2600, Loss: 19.423\n",
      "Epoch: 5.735, Batch: 2700, Loss: 20.290\n",
      "Epoch: 5.762, Batch: 2800, Loss: 20.552\n",
      "Epoch: 5.790, Batch: 2900, Loss: 19.218\n",
      "Epoch: 5.817, Batch: 3000, Loss: 21.195\n",
      "Epoch: 5.844, Batch: 3100, Loss: 17.509\n",
      "Epoch: 5.871, Batch: 3200, Loss: 23.949\n",
      "Epoch: 5.898, Batch: 3300, Loss: 23.563\n",
      "Epoch: 5.926, Batch: 3400, Loss: 11.621\n",
      "Epoch: 5.953, Batch: 3500, Loss: 14.918\n",
      "Epoch: 5.980, Batch: 3600, Loss: 19.962\n",
      "Epoch:  6 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.609, Train_acc: 0.703,\n",
      "Val_loss: 2.505, Val_acc: 0.716\n",
      "Epoch: 6.000, Batch: 0, Loss: 13.536\n",
      "Epoch: 6.027, Batch: 100, Loss: 21.908\n",
      "Epoch: 6.054, Batch: 200, Loss: 18.927\n",
      "Epoch: 6.082, Batch: 300, Loss: 17.207\n",
      "Epoch: 6.109, Batch: 400, Loss: 17.357\n",
      "Epoch: 6.136, Batch: 500, Loss: 19.426\n",
      "Epoch: 6.163, Batch: 600, Loss: 24.000\n",
      "Epoch: 6.191, Batch: 700, Loss: 17.158\n",
      "Epoch: 6.218, Batch: 800, Loss: 18.710\n",
      "Epoch: 6.245, Batch: 900, Loss: 24.721\n",
      "Epoch: 6.272, Batch: 1000, Loss: 20.333\n",
      "Epoch: 6.299, Batch: 1100, Loss: 22.639\n",
      "Epoch: 6.327, Batch: 1200, Loss: 19.914\n",
      "Epoch: 6.354, Batch: 1300, Loss: 20.253\n",
      "Epoch: 6.381, Batch: 1400, Loss: 15.253\n",
      "Epoch: 6.408, Batch: 1500, Loss: 19.764\n",
      "Epoch: 6.436, Batch: 1600, Loss: 17.484\n",
      "Epoch: 6.463, Batch: 1700, Loss: 13.973\n",
      "Epoch: 6.490, Batch: 1800, Loss: 22.499\n",
      "Epoch: 6.517, Batch: 1900, Loss: 21.198\n",
      "Epoch: 6.545, Batch: 2000, Loss: 18.610\n",
      "Epoch: 6.572, Batch: 2100, Loss: 21.205\n",
      "Epoch: 6.599, Batch: 2200, Loss: 18.472\n",
      "Epoch: 6.626, Batch: 2300, Loss: 20.789\n",
      "Epoch: 6.653, Batch: 2400, Loss: 18.606\n",
      "Epoch: 6.681, Batch: 2500, Loss: 15.948\n",
      "Epoch: 6.708, Batch: 2600, Loss: 26.345\n",
      "Epoch: 6.735, Batch: 2700, Loss: 17.714\n",
      "Epoch: 6.762, Batch: 2800, Loss: 23.721\n",
      "Epoch: 6.790, Batch: 2900, Loss: 21.015\n",
      "Epoch: 6.817, Batch: 3000, Loss: 20.818\n",
      "Epoch: 6.844, Batch: 3100, Loss: 18.836\n",
      "Epoch: 6.871, Batch: 3200, Loss: 22.049\n",
      "Epoch: 6.898, Batch: 3300, Loss: 22.287\n",
      "Epoch: 6.926, Batch: 3400, Loss: 19.756\n",
      "Epoch: 6.953, Batch: 3500, Loss: 19.434\n",
      "Epoch: 6.980, Batch: 3600, Loss: 16.303\n",
      "Epoch:  7 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.555, Train_acc: 0.705,\n",
      "Val_loss: 2.515, Val_acc: 0.715\n",
      "Epoch: 7.000, Batch: 0, Loss: 17.481\n",
      "Epoch: 7.027, Batch: 100, Loss: 13.518\n",
      "Epoch: 7.054, Batch: 200, Loss: 17.799\n",
      "Epoch: 7.082, Batch: 300, Loss: 21.652\n",
      "Epoch: 7.109, Batch: 400, Loss: 18.297\n",
      "Epoch: 7.136, Batch: 500, Loss: 16.886\n",
      "Epoch: 7.163, Batch: 600, Loss: 16.400\n",
      "Epoch: 7.191, Batch: 700, Loss: 19.938\n",
      "Epoch: 7.218, Batch: 800, Loss: 19.346\n",
      "Epoch: 7.245, Batch: 900, Loss: 16.207\n",
      "Epoch: 7.272, Batch: 1000, Loss: 18.764\n",
      "Epoch: 7.299, Batch: 1100, Loss: 21.665\n",
      "Epoch: 7.327, Batch: 1200, Loss: 23.159\n",
      "Epoch: 7.354, Batch: 1300, Loss: 17.343\n",
      "Epoch: 7.381, Batch: 1400, Loss: 22.480\n",
      "Epoch: 7.408, Batch: 1500, Loss: 20.281\n",
      "Epoch: 7.436, Batch: 1600, Loss: 16.390\n",
      "Epoch: 7.463, Batch: 1700, Loss: 20.097\n",
      "Epoch: 7.490, Batch: 1800, Loss: 19.062\n",
      "Epoch: 7.517, Batch: 1900, Loss: 17.840\n",
      "Epoch: 7.545, Batch: 2000, Loss: 22.581\n",
      "Epoch: 7.572, Batch: 2100, Loss: 17.409\n",
      "Epoch: 7.599, Batch: 2200, Loss: 25.952\n",
      "Epoch: 7.626, Batch: 2300, Loss: 21.311\n",
      "Epoch: 7.653, Batch: 2400, Loss: 28.304\n",
      "Epoch: 7.681, Batch: 2500, Loss: 9.565\n",
      "Epoch: 7.708, Batch: 2600, Loss: 22.214\n",
      "Epoch: 7.735, Batch: 2700, Loss: 21.986\n",
      "Epoch: 7.762, Batch: 2800, Loss: 18.535\n",
      "Epoch: 7.790, Batch: 2900, Loss: 22.799\n",
      "Epoch: 7.817, Batch: 3000, Loss: 22.016\n",
      "Epoch: 7.844, Batch: 3100, Loss: 17.911\n",
      "Epoch: 7.871, Batch: 3200, Loss: 15.939\n",
      "Epoch: 7.898, Batch: 3300, Loss: 19.567\n",
      "Epoch: 7.926, Batch: 3400, Loss: 17.018\n",
      "Epoch: 7.953, Batch: 3500, Loss: 20.834\n",
      "Epoch: 7.980, Batch: 3600, Loss: 19.010\n",
      "Epoch:  8 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.512, Train_acc: 0.706,\n",
      "Val_loss: 2.595, Val_acc: 0.716\n",
      "Epoch: 8.000, Batch: 0, Loss: 15.434\n",
      "Epoch: 8.027, Batch: 100, Loss: 20.146\n",
      "Epoch: 8.054, Batch: 200, Loss: 21.986\n",
      "Epoch: 8.082, Batch: 300, Loss: 15.977\n",
      "Epoch: 8.109, Batch: 400, Loss: 14.249\n",
      "Epoch: 8.136, Batch: 500, Loss: 19.987\n",
      "Epoch: 8.163, Batch: 600, Loss: 16.353\n",
      "Epoch: 8.191, Batch: 700, Loss: 23.486\n",
      "Epoch: 8.218, Batch: 800, Loss: 15.623\n",
      "Epoch: 8.245, Batch: 900, Loss: 26.254\n",
      "Epoch: 8.272, Batch: 1000, Loss: 18.633\n",
      "Epoch: 8.299, Batch: 1100, Loss: 20.541\n",
      "Epoch: 8.327, Batch: 1200, Loss: 15.373\n",
      "Epoch: 8.354, Batch: 1300, Loss: 16.188\n",
      "Epoch: 8.381, Batch: 1400, Loss: 17.310\n",
      "Epoch: 8.408, Batch: 1500, Loss: 17.574\n",
      "Epoch: 8.436, Batch: 1600, Loss: 18.515\n",
      "Epoch: 8.463, Batch: 1700, Loss: 18.320\n",
      "Epoch: 8.490, Batch: 1800, Loss: 20.447\n",
      "Epoch: 8.517, Batch: 1900, Loss: 20.044\n",
      "Epoch: 8.545, Batch: 2000, Loss: 17.326\n",
      "Epoch: 8.572, Batch: 2100, Loss: 19.246\n",
      "Epoch: 8.599, Batch: 2200, Loss: 20.796\n",
      "Epoch: 8.626, Batch: 2300, Loss: 16.353\n",
      "Epoch: 8.653, Batch: 2400, Loss: 14.933\n",
      "Epoch: 8.681, Batch: 2500, Loss: 18.717\n",
      "Epoch: 8.708, Batch: 2600, Loss: 26.218\n",
      "Epoch: 8.735, Batch: 2700, Loss: 16.653\n",
      "Epoch: 8.762, Batch: 2800, Loss: 21.759\n",
      "Epoch: 8.790, Batch: 2900, Loss: 16.210\n",
      "Epoch: 8.817, Batch: 3000, Loss: 14.186\n",
      "Epoch: 8.844, Batch: 3100, Loss: 17.419\n",
      "Epoch: 8.871, Batch: 3200, Loss: 16.255\n",
      "Epoch: 8.898, Batch: 3300, Loss: 24.111\n",
      "Epoch: 8.926, Batch: 3400, Loss: 17.256\n",
      "Epoch: 8.953, Batch: 3500, Loss: 20.679\n",
      "Epoch: 8.980, Batch: 3600, Loss: 17.681\n",
      "Epoch:  9 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.478, Train_acc: 0.708,\n",
      "Val_loss: 2.527, Val_acc: 0.701\n",
      "Epoch: 9.000, Batch: 0, Loss: 18.605\n",
      "Epoch: 9.027, Batch: 100, Loss: 14.989\n",
      "Epoch: 9.054, Batch: 200, Loss: 20.657\n",
      "Epoch: 9.082, Batch: 300, Loss: 20.524\n",
      "Epoch: 9.109, Batch: 400, Loss: 22.611\n",
      "Epoch: 9.136, Batch: 500, Loss: 20.239\n",
      "Epoch: 9.163, Batch: 600, Loss: 19.798\n",
      "Epoch: 9.191, Batch: 700, Loss: 16.896\n",
      "Epoch: 9.218, Batch: 800, Loss: 20.335\n",
      "Epoch: 9.245, Batch: 900, Loss: 21.384\n",
      "Epoch: 9.272, Batch: 1000, Loss: 26.617\n",
      "Epoch: 9.299, Batch: 1100, Loss: 21.349\n",
      "Epoch: 9.327, Batch: 1200, Loss: 11.197\n",
      "Epoch: 9.354, Batch: 1300, Loss: 19.213\n",
      "Epoch: 9.381, Batch: 1400, Loss: 19.686\n",
      "Epoch: 9.408, Batch: 1500, Loss: 19.753\n",
      "Epoch: 9.436, Batch: 1600, Loss: 21.005\n",
      "Epoch: 9.463, Batch: 1700, Loss: 16.184\n",
      "Epoch: 9.490, Batch: 1800, Loss: 22.813\n",
      "Epoch: 9.517, Batch: 1900, Loss: 18.964\n",
      "Epoch: 9.545, Batch: 2000, Loss: 17.162\n",
      "Epoch: 9.572, Batch: 2100, Loss: 13.063\n",
      "Epoch: 9.599, Batch: 2200, Loss: 22.639\n",
      "Epoch: 9.626, Batch: 2300, Loss: 19.404\n",
      "Epoch: 9.653, Batch: 2400, Loss: 22.704\n",
      "Epoch: 9.681, Batch: 2500, Loss: 17.449\n",
      "Epoch: 9.708, Batch: 2600, Loss: 17.318\n",
      "Epoch: 9.735, Batch: 2700, Loss: 16.636\n",
      "Epoch: 9.762, Batch: 2800, Loss: 19.801\n",
      "Epoch: 9.790, Batch: 2900, Loss: 22.050\n",
      "Epoch: 9.817, Batch: 3000, Loss: 27.166\n",
      "Epoch: 9.844, Batch: 3100, Loss: 21.465\n",
      "Epoch: 9.871, Batch: 3200, Loss: 20.128\n",
      "Epoch: 9.898, Batch: 3300, Loss: 16.921\n",
      "Epoch: 9.926, Batch: 3400, Loss: 15.613\n",
      "Epoch: 9.953, Batch: 3500, Loss: 20.041\n",
      "Epoch: 9.980, Batch: 3600, Loss: 15.782\n",
      "Epoch:  10 / 10 \n",
      "----------------------------\n",
      "Train_loss: 1.449, Train_acc: 0.709,\n",
      "Val_loss: 2.527, Val_acc: 0.703\n",
      "CPU times: user 2h 12min 43s, sys: 45min 18s, total: 2h 58min 2s\n",
      "Wall time: 2h 59min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# after pin_memory and non-blocking\n",
    "train_amp(model3, optimizer, train_kitti_amp, val_kitti_amp, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "responsible-pixel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyKitti_model_v2(\n",
       "  (features1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (features2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       "  (bbox): Sequential(\n",
       "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tuple = load_model('model/model_amp_test2_epoch_9', model3, optimizer)\n",
    "loaded_model = model_tuple[0]\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-harmony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
