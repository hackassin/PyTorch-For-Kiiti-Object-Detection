{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atomic-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np, pandas as pd\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from utils import helper as hp\n",
    "from utils import ssd_helper as ssdhp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook as tqnb\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-irrigation",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cbc75a64d5466e8ce2e47dee78d2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Preparing dataframe'), FloatProgress(value=0.0, max=12903.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>classes</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person]</td>\n",
       "      <td>[[182.7, 103.02, 366.45, 358.02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>[[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[bicycle, bicycle, bicycle, person, person, pe...</td>\n",
       "      <td>[[107.1, 206.04, 390.15, 510.0], [384.03, 246....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>[[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person]</td>\n",
       "      <td>[[38.5, 72.42, 468.16, 510.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/im...   \n",
       "1  data/kitti/integration/resized_aug/training/im...   \n",
       "2  data/kitti/integration/resized_aug/training/im...   \n",
       "3  data/kitti/integration/resized_aug/training/im...   \n",
       "4  data/kitti/integration/resized_aug/training/im...   \n",
       "\n",
       "                                          label_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/la...   \n",
       "1  data/kitti/integration/resized_aug/training/la...   \n",
       "2  data/kitti/integration/resized_aug/training/la...   \n",
       "3  data/kitti/integration/resized_aug/training/la...   \n",
       "4  data/kitti/integration/resized_aug/training/la...   \n",
       "\n",
       "                                             classes  \\\n",
       "0                                           [person]   \n",
       "1                                   [person, person]   \n",
       "2  [bicycle, bicycle, bicycle, person, person, pe...   \n",
       "3                                   [person, person]   \n",
       "4                                           [person]   \n",
       "\n",
       "                                              bboxes  \n",
       "0                  [[182.7, 103.02, 366.45, 358.02]]  \n",
       "1  [[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...  \n",
       "2  [[107.1, 206.04, 390.15, 510.0], [384.03, 246....  \n",
       "3  [[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...  \n",
       "4                     [[38.5, 72.42, 468.16, 510.0]]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impath = 'data/kitti/integration/resized_aug/training/images/'\n",
    "labels_path = 'data/kitti/integration/resized_aug/training/labels/'\n",
    "imlabel_list = hp.imlabel(impath, labels_path)\n",
    "df = pd.DataFrame(columns = ['image_path','label_path','classes','bboxes'])\n",
    "# df = pd.DataFrame()\n",
    "# df['image_path'] = imlabel_list[:][0]\n",
    "# df['label_path'] = imlabel_list[:][1]\n",
    "# df.head()\n",
    "\n",
    "for item,bar in zip(imlabel_list,tqnb(range(len(imlabel_list)),desc='Preparing dataframe')):\n",
    "    # print(item)\n",
    "    #bboxes = helper.fetch_bboxes(item[1]).tolist()\n",
    "    bboxes,classes = hp.fetch_bbox_lb(item[1], augflag=True)\n",
    "    df = df.append({'image_path':item[0], 'label_path': item[1],\n",
    "                    'classes': classes, 'bboxes': bboxes}, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endless-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'car': 0, \n",
    "              'bus': 1, 'motorbike': 2,\n",
    "              'bicycle': 3, 'cat': 4,\n",
    "             'person': 5}\n",
    "\n",
    "def num_label(class_dict, classes):\n",
    "    classes = [class_dict[x] for x in classes]\n",
    "    return np.array(classes)\n",
    "\n",
    "df['classes'] = df.apply(lambda x : num_label(class_dict, x['classes']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-package",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>classes</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[[182.7, 103.02, 366.45, 358.02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[3, 3, 3, 5, 5, 5]</td>\n",
       "      <td>[[107.1, 206.04, 390.15, 510.0], [384.03, 246....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[[38.5, 72.42, 468.16, 510.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/im...   \n",
       "1  data/kitti/integration/resized_aug/training/im...   \n",
       "2  data/kitti/integration/resized_aug/training/im...   \n",
       "3  data/kitti/integration/resized_aug/training/im...   \n",
       "4  data/kitti/integration/resized_aug/training/im...   \n",
       "\n",
       "                                          label_path             classes  \\\n",
       "0  data/kitti/integration/resized_aug/training/la...                 [5]   \n",
       "1  data/kitti/integration/resized_aug/training/la...              [5, 5]   \n",
       "2  data/kitti/integration/resized_aug/training/la...  [3, 3, 3, 5, 5, 5]   \n",
       "3  data/kitti/integration/resized_aug/training/la...              [5, 5]   \n",
       "4  data/kitti/integration/resized_aug/training/la...                 [5]   \n",
       "\n",
       "                                              bboxes  \n",
       "0                  [[182.7, 103.02, 366.45, 358.02]]  \n",
       "1  [[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...  \n",
       "2  [[107.1, 206.04, 390.15, 510.0], [384.03, 246....  \n",
       "3  [[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...  \n",
       "4                     [[38.5, 72.42, 468.16, 510.0]]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-lithuania",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VOC_LABELS = ('aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person',\n",
    "        'pottedplant','sheep','sofa','train','tvmonitor',)\"\"\"\n",
    "\n",
    "# DET_LABELS = ['car', 'bus', 'motorbike', 'bicycle', 'cat', 'person']\n",
    "\n",
    "\n",
    "class ObjDetection(data.Dataset):\n",
    "\n",
    "    def __init__(self, opt, df, is_train=True):\n",
    "         self.data = df\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data.loc[index,'image_path'].values\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        boxes, labels = self.data.loc[index, ['bboxes', 'classes']].values\n",
    "        return image, boxes, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-modification",
   "metadata": {},
   "source": [
    "### Building ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.downsample(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "#         print(out.size())\n",
    "        out = self.maxpool(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer1(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer2(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer3(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer4(out)\n",
    "#         print(out.size())\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-singer",
   "metadata": {},
   "source": [
    "### Building SSD with Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class L2Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels, scale):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale or None\n",
    "        self.eps = 1e-10\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight, self.gamma)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(x.pow(2).sum(dim=1, keepdim=True)) + self.eps\n",
    "        x = torch.div(x, norm)\n",
    "        x = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
    "        return x    \n",
    "\n",
    "    \n",
    "def extra():\n",
    "    layers = []\n",
    "    conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1)\n",
    "    conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "    conv9_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "    conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "    conv10_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
    "    conv10_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "    conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "    conv11_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "\n",
    "    layers = [conv8_1, conv8_2, conv9_1, conv9_2, conv10_1, conv10_2, conv11_1, conv11_2]\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feature_extractor(ver, extral, bboxes, num_classes):\n",
    "    \n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    \n",
    "    if ver == 'RES18_SSD':\n",
    "        loc_layers += [nn.Conv2d(128, bboxes[0] * 4, kernel_size=3, padding=1)]\n",
    "        loc_layers += [nn.Conv2d(256, bboxes[1] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(128, bboxes[0] * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(256, bboxes[1] * num_classes, kernel_size=3, padding=1)]\n",
    "    \n",
    "    \n",
    "    for k, v in enumerate(extral[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "        \n",
    "    \n",
    "    return loc_layers, conf_layers \n",
    "\n",
    "\n",
    "class RES18_SSD(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, bboxes, pretrained=None):\n",
    "        super(RES18_SSD, self).__init__()\n",
    "        \n",
    "        self.ver = 'RES18_SSD'\n",
    "        self.num_classes = num_classes\n",
    "        self.bboxes = bboxes      \n",
    "        self.extra_list = extra()\n",
    "        self.loc_layers_list, self.conf_layers_list = feature_extractor(self.ver, self.extra_list, self.bboxes, self.num_classes)\n",
    "        self.L2Norm = L2Norm(128, 20)\n",
    "\n",
    "\n",
    "        resnet = ResNet18()\n",
    "        if pretrained:\n",
    "            net = torch.load('./weights/newresnet.pth')\n",
    "            print('resnet18 pretrain_model loading...')\n",
    "            resnet.load_state_dict(net)\n",
    "        \n",
    "        self.res = nn.Sequential(\n",
    "            *list(resnet.children())[:-2],\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.extras = nn.ModuleList(self.extra_list)\n",
    "        self.loc = nn.ModuleList(self.loc_layers_list)\n",
    "        self.conf = nn.ModuleList(self.conf_layers_list)\n",
    "        \n",
    "        \n",
    "#  xavier initialization\n",
    "#         layers = [self.extras, self.loc, self.conf]\n",
    "#         print(self.vgg)\n",
    "#         for i in layers:\n",
    "#             for m in i.modules():\n",
    "#                 if isinstance(m, nn.Conv2d):\n",
    "#                     nn.init.xavier_uniform_(m.weight)\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        source = []\n",
    "        loc = []\n",
    "        conf = []\n",
    "        res_source = [5, 6]\n",
    "        for i, v in enumerate(self.res):\n",
    "            x = v(x)\n",
    "            if i in res_source:\n",
    "                if i == 5:\n",
    "                    s = self.L2Norm(x)\n",
    "                else:\n",
    "                    s = x\n",
    "                source.append(s)\n",
    "\n",
    "        for i, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if i % 2 == 1:\n",
    "                source.append(x)\n",
    "\n",
    "\n",
    "        for s, l, c in zip(source, self.loc, self.conf):\n",
    "            loc.append(l(s).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(s).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "       \n",
    "\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "        return loc, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxEncoder(object):\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        self.variance = opt.variance\n",
    "        default_boxes = list()\n",
    "        \n",
    "        for k in range(len(opt.grids)):\n",
    "            for v, u in itertools.product(range(opt.grids[k]), repeat=2):\n",
    "                cx = (u + 0.5) * opt.steps[k]\n",
    "                cy = (v + 0.5) * opt.steps[k]\n",
    "\n",
    "                s = opt.sizes[k]\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = np.sqrt(opt.sizes[k] * opt.sizes[k + 1])\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = opt.sizes[k]\n",
    "                for ar in opt.aspect_ratios[k]:\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s * np.sqrt(ar), s / np.sqrt(ar)))\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s / np.sqrt(ar), s * np.sqrt(ar)))\n",
    "\n",
    "        default_boxes = np.clip(default_boxes, a_min=0, a_max=1)\n",
    "        self.default_boxes = np.array(default_boxes)\n",
    "\n",
    "    def encode(self, boxes, labels, threshold=0.5):\n",
    "       \n",
    "        if len(boxes) == 0:\n",
    "            return (\n",
    "                np.zeros(self.default_boxes.shape, dtype=np.float32),\n",
    "                np.zeros(self.default_boxes.shape[:1], dtype=np.int32))\n",
    "\n",
    "        iou = bbox_iou(point_form(self.default_boxes), boxes)\n",
    "\n",
    "\n",
    "        gt_idx = iou.argmax(axis=1)\n",
    "        iou = iou.max(axis=1)\n",
    "        boxes = boxes[gt_idx]\n",
    "        labels = labels[gt_idx]\n",
    "\n",
    "        loc = np.hstack((\n",
    "            ((boxes[:, :2] + boxes[:, 2:]) / 2 - self.default_boxes[:, :2]) /\n",
    "            (self.variance[0] * self.default_boxes[:, 2:]),\n",
    "            np.log((boxes[:, 2:] - boxes[:, :2]) / self.default_boxes[:, 2:]) /\n",
    "            self.variance[1]))\n",
    "\n",
    "        conf = 1 + labels\n",
    "        conf[iou < threshold] = 0\n",
    "       \n",
    "\n",
    "        return loc.astype(np.float32), conf.astype(np.int32)\n",
    "\n",
    "    def decode(self, loc):\n",
    "        \n",
    "        boxes = np.hstack((\n",
    "            self.default_boxes[:, :2] +\n",
    "            loc[:, :2] * self.variance[0] * self.default_boxes[:, 2:],\n",
    "            self.default_boxes[:, 2:] * np.exp(loc[:, 2:] * self.variance[1])))\n",
    "        boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negtives(logits, labels, pos, neg_radio):\n",
    "    \n",
    "    \n",
    "    num_batch, num_anchors, num_classes = logits.shape\n",
    "    logits = logits.view(-1, num_classes)\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    losses = losses.view(num_batch, num_anchors)\n",
    "\n",
    "    losses[pos] = 0\n",
    "\n",
    "    \n",
    "    loss_idx = losses.argsort(1, descending=True)\n",
    "    rank = loss_idx.argsort(1) \n",
    "\n",
    "    num_pos = pos.long().sum(1, keepdim=True)\n",
    "    num_neg = torch.clamp(neg_radio*num_pos, max=pos.shape[1]-1) #(batch, 1)\n",
    "    neg = rank < num_neg.expand_as(rank)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neg\n",
    "    \n",
    "class MultiBoxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, neg_radio=3):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.neg_radio = neg_radio\n",
    "    \n",
    "    def forward(self, pred_loc, pred_label, gt_loc, gt_label):\n",
    "        \n",
    "\n",
    "        num_batch = pred_loc.shape[0]\n",
    "\n",
    "        pos_idx = gt_label > 0\n",
    "        pos_loc_idx = pos_idx.unsqueeze(2).expand_as(pred_loc)\n",
    "        pred_loc_pos = pred_loc[pos_loc_idx].view(-1, 4)\n",
    "        gt_loc_pos = gt_loc[pos_loc_idx].view(-1, 4)\n",
    "\n",
    "        loc_loss = F.smooth_l1_loss(pred_loc_pos, gt_loc_pos, reduction='sum')\n",
    "\n",
    "        \n",
    "        logits = pred_label.detach()\n",
    "        labels = gt_label.detach()\n",
    "        neg_idx = hard_negtives(logits, labels, pos_idx, self.neg_radio) #neg (batch, n)\n",
    "\n",
    "        pos_cls_mask = pos_idx.unsqueeze(2).expand_as(pred_label)\n",
    "        neg_cls_mask = neg_idx.unsqueeze(2).expand_as(pred_label)\n",
    "\n",
    "        conf_p = pred_label[(pos_cls_mask+neg_cls_mask).gt(0)].view(-1, self.num_classes)\n",
    "        target = gt_label[(pos_idx+neg_idx).gt(0)]\n",
    "\n",
    "        cls_loss = F.cross_entropy(conf_p, target, reduction='sum')\n",
    "        N = pos_idx.long().sum()\n",
    "\n",
    "        loc_loss /= N\n",
    "        cls_loss /= N\n",
    "\n",
    "\n",
    "        return loc_loss, cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter config\n",
    "class Config:\n",
    "    #voc root\n",
    "    # VOC_ROOT = '/SSD_ResNet_Pytorch'\n",
    "\n",
    "    #class + 1\n",
    "    num_classes = 6\n",
    "    #learning rate\n",
    "    lr = 0.001\n",
    "    #ssd paper = 32\n",
    "    batch_size = 32 \n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "    # 40k + 10k = 116 epock\n",
    "    epoch = 116 \n",
    "    #pre-train VGG root\n",
    "    #The resnet pre-train model is in lib.res-model...\n",
    "    save_folder = './weights/'\n",
    "    # basenet = 'vgg16_reducedfc.pth'\n",
    "    log_fn = 10 \n",
    "    neg_ratio = 3   \n",
    "    #input-image size\n",
    "    min_size = 512\n",
    "    #boxe out image size\n",
    "    grids = (38, 19, 10, 5, 3, 1)\n",
    "    #boxes num\n",
    "    anchor_num = [4, 6, 6, 6, 4, 4]\n",
    "    #255 * R, G, B\n",
    "    mean = (104, 117, 123)\n",
    "    aspect_ratios = ((2,), (2, 3), (2, 3), (2, 3), (2,), (2,))\n",
    "    steps = [s / 300 for s in (8, 16, 32, 64, 100, 300)]\n",
    "    sizes = [s / 300 for s in (30, 60, 111, 162, 213, 264, 315)] \n",
    "    variance = (0.1, 0.2)\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def adjust_learning_rate1(optimizer):\n",
    "    lr = opt.lr * 0.1\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def adjust_learning_rate2(optimizer):\n",
    "    lr = opt.lr * 0.01\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RES18_SSD(opt.num_classes, opt.anchor_num, pretrained=False).to(device)\n",
    "model.train()\n",
    "\n",
    "mb = MultiBoxEncoder(opt)\n",
    "\n",
    "# image_sets = [['2007', 'trainval'], ['2012', 'trainval']]\n",
    "# dataset = VOCDetection(opt, image_sets=image_sets, is_train=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, collate_fn=detection_collate, num_workers=12)\n",
    "\n",
    "criterion = MultiBoxLoss(opt.num_classes, opt.neg_radio).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=opt.lr, momentum=opt.momentum,weight_decay=opt.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for e in range(opt.epoch):\n",
    "        if e == 77:\n",
    "            adjust_learning_rate1(optimizer)\n",
    "        elif e == 96:\n",
    "            adjust_learning_rate2(optimizer)\n",
    "        total_loc_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_loss = 0\n",
    "        for i , (img, boxes, labels) in enumerate(dataloader):\n",
    "            img = img.to(device)\n",
    "            gt_boxes = []\n",
    "            gt_labels = []\n",
    "            for j, box in enumerate(boxes):\n",
    "                # labels = box[:, 4]\n",
    "                label = labels[j]\n",
    "                # box = box[:, :-1]\n",
    "                match_loc, match_label = mb.encode(box, labels)\n",
    "            \n",
    "                gt_boxes.append(match_loc)\n",
    "                gt_labels.append(match_label)\n",
    "            \n",
    "            gt_boxes = torch.FloatTensor(gt_boxes).to(device)\n",
    "            gt_labels = torch.LongTensor(gt_labels).to(device)\n",
    "\n",
    "\n",
    "            p_loc, p_label = model(img)\n",
    "\n",
    "\n",
    "            loc_loss, cls_loss = criterion(p_loc, p_label, gt_boxes, gt_labels)\n",
    "\n",
    "            loss = loc_loss + cls_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loc_loss += loc_loss.item()\n",
    "            total_cls_loss += cls_loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % opt.log_fn == 0:\n",
    "                avg_loc = total_loc_loss / (i+1)\n",
    "                avg_cls = total_cls_loss / (i+1)\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print('epoch[{}] | batch_idx[{}] | loc_loss [{:.2f}] | cls_loss [{:.2f}] | total_loss [{:.2f}]'.format(e, i, avg_loc, avg_cls, avg_loss))\n",
    "        if e > 100:\n",
    "            torch.save(model.state_dict(), os.path.join(opt.save_folder, 'loss-{:.2f}.pth'.format(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-venue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
