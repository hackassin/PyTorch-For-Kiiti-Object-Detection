{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "residential-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from utils import ssd_helper as ssdhp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instructional-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.downsample(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "#         print(out.size())\n",
    "        out = self.maxpool(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer1(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer2(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer3(out)\n",
    "#         print(out.size())\n",
    "        out = self.layer4(out)\n",
    "#         print(out.size())\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contemporary-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels, scale):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale or None\n",
    "        self.eps = 1e-10\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight, self.gamma)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(x.pow(2).sum(dim=1, keepdim=True)) + self.eps\n",
    "        x = torch.div(x, norm)\n",
    "        x = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
    "        return x    \n",
    "    \n",
    "def extra():\n",
    "    layers = []\n",
    "    conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1)\n",
    "    conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "    conv9_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "    conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "    conv10_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
    "    conv10_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "    conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "    conv11_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "\n",
    "    layers = [conv8_1, conv8_2, conv9_1, conv9_2, conv10_1, conv10_2, conv11_1, conv11_2]\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feature_extractor(ver, extral, bboxes, num_classes):\n",
    "    \n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    \n",
    "    if ver == 'RES18_SSD':\n",
    "        loc_layers += [nn.Conv2d(128, bboxes[0] * 4, kernel_size=3, padding=1)]\n",
    "        loc_layers += [nn.Conv2d(256, bboxes[1] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(128, bboxes[0] * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(256, bboxes[1] * num_classes, kernel_size=3, padding=1)]\n",
    "    \n",
    "    \n",
    "    for k, v in enumerate(extral[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "        \n",
    "    \n",
    "    return loc_layers, conf_layers \n",
    "\n",
    "\n",
    "class RES18_SSD(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, bboxes, pretrained=None ):\n",
    "        super(RES18_SSD, self).__init__()\n",
    "        \n",
    "        self.ver = 'RES18_SSD'\n",
    "        self.num_classes = num_classes\n",
    "        self.bboxes = bboxes      \n",
    "        self.extra_list = extra()\n",
    "        self.loc_layers_list, self.conf_layers_list = feature_extractor(self.ver, self.extra_list, self.bboxes, self.num_classes)\n",
    "        self.L2Norm = L2Norm(128, 20)\n",
    "\n",
    "\n",
    "        resnet = ResNet18()\n",
    "        if pretrained:\n",
    "            net = torch.load('./weights/newresnet.pth')\n",
    "            print('resnet18 pretrain_model loading...')\n",
    "            resnet.load_state_dict(net)\n",
    "        \n",
    "        self.res = nn.Sequential(\n",
    "            *list(resnet.children())[:-2],\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.extras = nn.ModuleList(self.extra_list)\n",
    "        self.loc = nn.ModuleList(self.loc_layers_list)\n",
    "        self.conf = nn.ModuleList(self.conf_layers_list)\n",
    "        \n",
    "        \n",
    "#  xavier initialization\n",
    "#         layers = [self.extras, self.loc, self.conf]\n",
    "#         print(self.vgg)\n",
    "#         for i in layers:\n",
    "#             for m in i.modules():\n",
    "#                 if isinstance(m, nn.Conv2d):\n",
    "#                     nn.init.xavier_uniform_(m.weight)\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        source = []\n",
    "        loc = []\n",
    "        conf = []\n",
    "        res_source = [5, 6]\n",
    "        for i, v in enumerate(self.res):\n",
    "            x = v(x)\n",
    "            if i in res_source:\n",
    "                if i == 5:\n",
    "                    s = self.L2Norm(x)\n",
    "                else:\n",
    "                    s = x\n",
    "                source.append(s)\n",
    "\n",
    "        for i, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if i % 2 == 1:\n",
    "                source.append(x)\n",
    "\n",
    "\n",
    "        for s, l, c in zip(source, self.loc, self.conf):\n",
    "            loc.append(l(s).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(s).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "       \n",
    "\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "        return loc, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxEncoder(object):\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        self.variance = opt.variance\n",
    "        default_boxes = list()\n",
    "        \n",
    "        for k in range(len(opt.grids)):\n",
    "            for v, u in itertools.product(range(opt.grids[k]), repeat=2):\n",
    "                cx = (u + 0.5) * opt.steps[k]\n",
    "                cy = (v + 0.5) * opt.steps[k]\n",
    "\n",
    "                s = opt.sizes[k]\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = np.sqrt(opt.sizes[k] * opt.sizes[k + 1])\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = opt.sizes[k]\n",
    "                for ar in opt.aspect_ratios[k]:\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s * np.sqrt(ar), s / np.sqrt(ar)))\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s / np.sqrt(ar), s * np.sqrt(ar)))\n",
    "\n",
    "        default_boxes = np.clip(default_boxes, a_min=0, a_max=1)\n",
    "        self.default_boxes = np.array(default_boxes)\n",
    "\n",
    "    def encode(self, boxes, labels, threshold=0.5):\n",
    "       \n",
    "        if len(boxes) == 0:\n",
    "            return (\n",
    "                np.zeros(self.default_boxes.shape, dtype=np.float32),\n",
    "                np.zeros(self.default_boxes.shape[:1], dtype=np.int32))\n",
    "\n",
    "        iou = bbox_iou(point_form(self.default_boxes), boxes)\n",
    "\n",
    "\n",
    "        gt_idx = iou.argmax(axis=1)\n",
    "        iou = iou.max(axis=1)\n",
    "        boxes = boxes[gt_idx]\n",
    "        labels = labels[gt_idx]\n",
    "\n",
    "        loc = np.hstack((\n",
    "            ((boxes[:, :2] + boxes[:, 2:]) / 2 - self.default_boxes[:, :2]) /\n",
    "            (self.variance[0] * self.default_boxes[:, 2:]),\n",
    "            np.log((boxes[:, 2:] - boxes[:, :2]) / self.default_boxes[:, 2:]) /\n",
    "            self.variance[1]))\n",
    "\n",
    "        conf = 1 + labels\n",
    "        conf[iou < threshold] = 0\n",
    "       \n",
    "\n",
    "        return loc.astype(np.float32), conf.astype(np.int32)\n",
    "\n",
    "    def decode(self, loc):\n",
    "        \n",
    "        boxes = np.hstack((\n",
    "            self.default_boxes[:, :2] +\n",
    "            loc[:, :2] * self.variance[0] * self.default_boxes[:, 2:],\n",
    "            self.default_boxes[:, 2:] * np.exp(loc[:, 2:] * self.variance[1])))\n",
    "        boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negtives(logits, labels, pos, neg_radio):\n",
    "    \n",
    "    \n",
    "    num_batch, num_anchors, num_classes = logits.shape\n",
    "    logits = logits.view(-1, num_classes)\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    losses = losses.view(num_batch, num_anchors)\n",
    "\n",
    "    losses[pos] = 0\n",
    "\n",
    "    \n",
    "    loss_idx = losses.argsort(1, descending=True)\n",
    "    rank = loss_idx.argsort(1) \n",
    "\n",
    "    num_pos = pos.long().sum(1, keepdim=True)\n",
    "    num_neg = torch.clamp(neg_radio*num_pos, max=pos.shape[1]-1) #(batch, 1)\n",
    "    neg = rank < num_neg.expand_as(rank)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neg\n",
    "    \n",
    "class MultiBoxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, neg_radio=3):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.neg_radio = neg_radio\n",
    "    \n",
    "    def forward(self, pred_loc, pred_label, gt_loc, gt_label):\n",
    "        \n",
    "\n",
    "        num_batch = pred_loc.shape[0]\n",
    "\n",
    "        pos_idx = gt_label > 0\n",
    "        pos_loc_idx = pos_idx.unsqueeze(2).expand_as(pred_loc)\n",
    "        pred_loc_pos = pred_loc[pos_loc_idx].view(-1, 4)\n",
    "        gt_loc_pos = gt_loc[pos_loc_idx].view(-1, 4)\n",
    "\n",
    "        loc_loss = F.smooth_l1_loss(pred_loc_pos, gt_loc_pos, reduction='sum')\n",
    "\n",
    "        \n",
    "        logits = pred_label.detach()\n",
    "        labels = gt_label.detach()\n",
    "        neg_idx = hard_negtives(logits, labels, pos_idx, self.neg_radio) #neg (batch, n)\n",
    "\n",
    "        pos_cls_mask = pos_idx.unsqueeze(2).expand_as(pred_label)\n",
    "        neg_cls_mask = neg_idx.unsqueeze(2).expand_as(pred_label)\n",
    "\n",
    "        conf_p = pred_label[(pos_cls_mask+neg_cls_mask).gt(0)].view(-1, self.num_classes)\n",
    "        target = gt_label[(pos_idx+neg_idx).gt(0)]\n",
    "\n",
    "        cls_loss = F.cross_entropy(conf_p, target, reduction='sum')\n",
    "        N = pos_idx.long().sum()\n",
    "\n",
    "        loc_loss /= N\n",
    "        cls_loss /= N\n",
    "\n",
    "\n",
    "        return loc_loss, cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nutritional-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    #voc root\n",
    "    # VOC_ROOT = '/SSD_ResNet_Pytorch'\n",
    "\n",
    "    #class + 1\n",
    "    num_classes = 4\n",
    "    #learning rate\n",
    "    lr = 0.001\n",
    "    #ssd paper = 32\n",
    "    batch_size = 32 \n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "    # 40k + 10k = 116 epock\n",
    "    epoch = 116 \n",
    "    #pre-train VGG root\n",
    "    #The resnet pre-train model is in lib.res-model...\n",
    "    save_folder = './weights/'\n",
    "    # basenet = 'vgg16_reducedfc.pth'\n",
    "    log_fn = 10 \n",
    "    neg_ratio = 3   \n",
    "    #input-image size\n",
    "    min_size = 300\n",
    "    #boxe out image size\n",
    "    grids = (38, 19, 10, 5, 3, 1)\n",
    "    #boxes num\n",
    "    anchor_num = [4, 6, 6, 6, 4, 4]\n",
    "    #255 * R, G, B\n",
    "    mean = (104, 117, 123)\n",
    "    aspect_ratios = ((2,), (2, 3), (2, 3), (2, 3), (2,), (2,))\n",
    "    steps = [s / 300 for s in (8, 16, 32, 64, 100, 300)]\n",
    "    sizes = [s / 300 for s in (30, 60, 111, 162, 213, 264, 315)] \n",
    "    variance = (0.1, 0.2)\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parliamentary-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def adjust_learning_rate1(optimizer):\n",
    "    lr = opt.lr * 0.1\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def adjust_learning_rate2(optimizer):\n",
    "    lr = opt.lr * 0.01\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RES18_SSD(opt.num_classes, opt.anchor_num, pretrained=False).to(device)\n",
    "model.train()\n",
    "\n",
    "mb = MultiBoxEncoder(opt)\n",
    "\n",
    "# image_sets = [['2007', 'trainval'], ['2012', 'trainval']]\n",
    "# dataset = VOCDetection(opt, image_sets=image_sets, is_train=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, collate_fn=detection_collate, num_workers=4)\n",
    "\n",
    "criterion = MultiBoxLoss(opt.num_classes, opt.neg_radio).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=opt.lr, momentum=opt.momentum,weight_decay=opt.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for e in range(opt.epoch):\n",
    "        if e == 77:\n",
    "            adjust_learning_rate1(optimizer)\n",
    "        elif e == 96:\n",
    "            adjust_learning_rate2(optimizer)\n",
    "        total_loc_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_loss = 0\n",
    "        for i , (img, boxes) in enumerate(dataloader):\n",
    "            img = img.to(device)\n",
    "            gt_boxes = []\n",
    "            gt_labels = []\n",
    "            for box in boxes:\n",
    "                labels = box[:, 4]\n",
    "                box = box[:, :-1]\n",
    "                match_loc, match_label = mb.encode(box, labels)\n",
    "            \n",
    "                gt_boxes.append(match_loc)\n",
    "                gt_labels.append(match_label)\n",
    "            \n",
    "            gt_boxes = torch.FloatTensor(gt_boxes).to(device)\n",
    "            gt_labels = torch.LongTensor(gt_labels).to(device)\n",
    "\n",
    "\n",
    "            p_loc, p_label = model(img)\n",
    "\n",
    "\n",
    "            loc_loss, cls_loss = criterion(p_loc, p_label, gt_boxes, gt_labels)\n",
    "\n",
    "            loss = loc_loss + cls_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loc_loss += loc_loss.item()\n",
    "            total_cls_loss += cls_loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % opt.log_fn == 0:\n",
    "                avg_loc = total_loc_loss / (i+1)\n",
    "                avg_cls = total_cls_loss / (i+1)\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print('epoch[{}] | batch_idx[{}] | loc_loss [{:.2f}] | cls_loss [{:.2f}] | total_loss [{:.2f}]'.format(e, i, avg_loc, avg_cls, avg_loss))\n",
    "        if e > 100:\n",
    "            torch.save(model.state_dict(), os.path.join(opt.save_folder, 'loss-{:.2f}.pth'.format(total_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
