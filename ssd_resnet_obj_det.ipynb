{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electrical-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np, pandas as pd\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from utils import helper as hp\n",
    "from utils import ssd_helper as ssdhp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import os, cv2, itertools\n",
    "from tqdm.notebook import tqdm_notebook as tqnb\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-rendering",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stretch-looking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c472ec7fb04c8c898f3c895f6ef4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Preparing dataframe'), FloatProgress(value=0.0, max=12903.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>classes</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person]</td>\n",
       "      <td>[[182.7, 103.02, 366.45, 358.02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>[[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[bicycle, bicycle, bicycle, person, person, pe...</td>\n",
       "      <td>[[107.1, 206.04, 390.15, 510.0], [384.03, 246....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>[[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[person]</td>\n",
       "      <td>[[38.5, 72.42, 468.16, 510.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/im...   \n",
       "1  data/kitti/integration/resized_aug/training/im...   \n",
       "2  data/kitti/integration/resized_aug/training/im...   \n",
       "3  data/kitti/integration/resized_aug/training/im...   \n",
       "4  data/kitti/integration/resized_aug/training/im...   \n",
       "\n",
       "                                          label_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/la...   \n",
       "1  data/kitti/integration/resized_aug/training/la...   \n",
       "2  data/kitti/integration/resized_aug/training/la...   \n",
       "3  data/kitti/integration/resized_aug/training/la...   \n",
       "4  data/kitti/integration/resized_aug/training/la...   \n",
       "\n",
       "                                             classes  \\\n",
       "0                                           [person]   \n",
       "1                                   [person, person]   \n",
       "2  [bicycle, bicycle, bicycle, person, person, pe...   \n",
       "3                                   [person, person]   \n",
       "4                                           [person]   \n",
       "\n",
       "                                              bboxes  \n",
       "0                  [[182.7, 103.02, 366.45, 358.02]]  \n",
       "1  [[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...  \n",
       "2  [[107.1, 206.04, 390.15, 510.0], [384.03, 246....  \n",
       "3  [[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...  \n",
       "4                     [[38.5, 72.42, 468.16, 510.0]]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impath = 'data/kitti/integration/resized_aug/training/images/'\n",
    "labels_path = 'data/kitti/integration/resized_aug/training/labels/'\n",
    "imlabel_list = hp.imlabel(impath, labels_path)\n",
    "df = pd.DataFrame(columns = ['image_path','label_path','classes','bboxes'])\n",
    "# df = pd.DataFrame()\n",
    "# df['image_path'] = imlabel_list[:][0]\n",
    "# df['label_path'] = imlabel_list[:][1]\n",
    "# df.head()\n",
    "\n",
    "for item,bar in zip(imlabel_list,tqnb(range(len(imlabel_list)),desc='Preparing dataframe')):\n",
    "    # print(item)\n",
    "    #bboxes = helper.fetch_bboxes(item[1]).tolist()\n",
    "    bboxes,classes = hp.fetch_bbox_lb(item[1], augflag=True)\n",
    "    df = df.append({'image_path':item[0], 'label_path': item[1],\n",
    "                    'classes': classes, 'bboxes': bboxes}, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "former-dining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>classes</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[[182.7, 103.02, 366.45, 358.02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[3, 3, 3, 5, 5, 5]</td>\n",
       "      <td>[[107.1, 206.04, 390.15, 510.0], [384.03, 246....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kitti/integration/resized_aug/training/im...</td>\n",
       "      <td>data/kitti/integration/resized_aug/training/la...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[[38.5, 72.42, 468.16, 510.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  data/kitti/integration/resized_aug/training/im...   \n",
       "1  data/kitti/integration/resized_aug/training/im...   \n",
       "2  data/kitti/integration/resized_aug/training/im...   \n",
       "3  data/kitti/integration/resized_aug/training/im...   \n",
       "4  data/kitti/integration/resized_aug/training/im...   \n",
       "\n",
       "                                          label_path             classes  \\\n",
       "0  data/kitti/integration/resized_aug/training/la...                 [5]   \n",
       "1  data/kitti/integration/resized_aug/training/la...              [5, 5]   \n",
       "2  data/kitti/integration/resized_aug/training/la...  [3, 3, 3, 5, 5, 5]   \n",
       "3  data/kitti/integration/resized_aug/training/la...              [5, 5]   \n",
       "4  data/kitti/integration/resized_aug/training/la...                 [5]   \n",
       "\n",
       "                                              bboxes  \n",
       "0                  [[182.7, 103.02, 366.45, 358.02]]  \n",
       "1  [[198.9, 327.6, 217.26, 416.78], [26.52, 343.9...  \n",
       "2  [[107.1, 206.04, 390.15, 510.0], [384.03, 246....  \n",
       "3  [[3.06, 124.67, 43.86, 282.22], [4.08, 38.36, ...  \n",
       "4                     [[38.5, 72.42, 468.16, 510.0]]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'car': 0, \n",
    "              'bus': 1, 'motorbike': 2,\n",
    "              'bicycle': 3, 'cat': 4,\n",
    "             'person': 5}\n",
    "\n",
    "def num_label(class_dict, classes):\n",
    "    classes = [class_dict[x] for x in classes]\n",
    "    return np.array(classes)\n",
    "\n",
    "df['classes'] = df.apply(lambda x : num_label(class_dict, x['classes']),axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-description",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "million-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VOC_LABELS = ('aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person',\n",
    "        'pottedplant','sheep','sofa','train','tvmonitor',)\"\"\"\n",
    "\n",
    "# DET_LABELS = ['car', 'bus', 'motorbike', 'bicycle', 'cat', 'person']\n",
    "\n",
    "def det_collate(batch):\n",
    "    imgs = []\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        boxes.append(sample[1])\n",
    "        classes.append(sample[2])\n",
    "    # return np.array(imgs), np.array(boxes), np.array(classes)\n",
    "    return torch.transpose(torch.tensor(imgs),3,1), np.array(boxes, dtype=object), np.array(classes, dtype=object)\n",
    "\n",
    "class ObjDetDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, paths, bboxes, classes):\n",
    "        self.paths = paths.values\n",
    "        self.bboxes = bboxes.values\n",
    "        self.classes = classes.values\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # img_path = self.data.loc[index,'image_path']\n",
    "        impath = self.paths[index]\n",
    "        image = cv2.cvtColor(cv2.imread(impath).astype('float32'),\n",
    "                         cv2.COLOR_BGR2RGB)/255\n",
    "        boxes = self.bboxes[index]\n",
    "        labels = self.classes[index]\n",
    "        return image, boxes, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "magnetic-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Split\n",
    "X = df.image_path\n",
    "y = df[['bboxes', 'classes']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-hundred",
   "metadata": {},
   "source": [
    "### Building ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contained-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-b1e780194352>, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-b1e780194352>\"\u001b[0;36m, line \u001b[0;32m100\u001b[0m\n\u001b[0;31m    out = self.layer4(out)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.downsample(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\"\"\"VOC_LABELS = ('aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person',\n",
    "        'pottedplant','sheep','sofa','train','tvmonitor',)\"\"\"\n",
    "# DET_LABELS = ['car', 'bus', 'motorbike', 'bicycle', 'cat', 'person']\n",
    "def det_collate(batch):\n",
    "    imgs = []\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        boxes.append(sample[1])\n",
    "        classes.append(sample[2])\n",
    "    # return np.array(imgs), np.array(boxes), np.array(classes)\n",
    "    return torch.transpose(torch.tensor(imgs),3,1), np.array(boxes, dtype=object), np.array(classes, dtype=object)\n",
    "\n",
    "class ObjDetDataset(data.Dataset):\n",
    "    def __init__(self, paths, bboxes, classes):\n",
    "        self.paths = paths.values\n",
    "        self.bboxes = bboxes.values\n",
    "        self.classes = classes.values\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # img_path = self.data.loc[index,'image_path']\n",
    "        impath = self.paths[index]\n",
    "        image = cv2.cvtColor(cv2.imread(impath).astype('float32'),\n",
    "                         cv2.COLOR_BGR2RGB)/255\n",
    "        boxes = self.bboxes[index]\n",
    "        labels = self.classes[index]\n",
    "        return image, boxes, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-brown",
   "metadata": {},
   "source": [
    " ### Building SSD with Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "indie-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For feature normalization\n",
    "class L2Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels, scale):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale or None\n",
    "        self.eps = 1e-10\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight, self.gamma)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(x.pow(2).sum(dim=1, keepdim=True)) + self.eps\n",
    "        x = torch.div(x, norm)\n",
    "        x = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
    "        return x    \n",
    "\n",
    "# Auxillary Convolution Layers    \n",
    "def extra():\n",
    "    layers = []\n",
    "    conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1)\n",
    "    conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "    conv9_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "    conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "    conv10_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
    "    conv10_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "    conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "    conv11_2 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "\n",
    "    layers = [conv8_1, conv8_2, conv9_1, conv9_2, conv10_1, conv10_2, conv11_1, conv11_2]\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feature_extractor(ver, extral, bboxes, num_classes):\n",
    "    \n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    \n",
    "    if ver == 'RES18_SSD':\n",
    "        loc_layers += [nn.Conv2d(128, bboxes[0] * 4, kernel_size=3, padding=1)]\n",
    "        loc_layers += [nn.Conv2d(256, bboxes[1] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(128, bboxes[0] * num_classes, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(256, bboxes[1] * num_classes, kernel_size=3, padding=1)]\n",
    "    \n",
    "    \n",
    "    for k, v in enumerate(extral[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, bboxes[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "        \n",
    "    \n",
    "    return loc_layers, conf_layers \n",
    "\n",
    "\n",
    "class RES18_SSD(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, bboxes, pretrained=None):\n",
    "        super(RES18_SSD, self).__init__()\n",
    "        \n",
    "        self.ver = 'RES18_SSD'\n",
    "        self.num_classes = num_classes\n",
    "        self.bboxes = bboxes      \n",
    "        self.extra_list = extra()\n",
    "        self.loc_layers_list, self.conf_layers_list = feature_extractor(self.ver, self.extra_list, self.bboxes, self.num_classes)\n",
    "        self.L2Norm = L2Norm(128, 20)\n",
    "\n",
    "\n",
    "        resnet = ResNet18()\n",
    "        if pretrained:\n",
    "            net = torch.load('./weights/newresnet.pth')\n",
    "            print('resnet18 pretrain_model loading...')\n",
    "            resnet.load_state_dict(net)\n",
    "        \n",
    "        self.res = nn.Sequential(\n",
    "            *list(resnet.children())[:-2],\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.extras = nn.ModuleList(self.extra_list)\n",
    "        self.loc = nn.ModuleList(self.loc_layers_list)\n",
    "        self.conf = nn.ModuleList(self.conf_layers_list)\n",
    "        \n",
    "        \n",
    "#  xavier initialization\n",
    "#         layers = [self.extras, self.loc, self.conf]\n",
    "#         print(self.vgg)\n",
    "#         for i in layers:\n",
    "#             for m in i.modules():\n",
    "#                 if isinstance(m, nn.Conv2d):\n",
    "#                     nn.init.xavier_uniform_(m.weight)\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        source = []\n",
    "        loc = []\n",
    "        conf = []\n",
    "        res_source = [5, 6]\n",
    "        for i, v in enumerate(self.res):\n",
    "            print(\"In forward: x.shape\", x.shape)\n",
    "            x = v(x)\n",
    "            if i in res_source:\n",
    "                if i == 5:\n",
    "                    s = self.L2Norm(x)\n",
    "                else:\n",
    "                    s = x\n",
    "                source.append(s)\n",
    "\n",
    "        for i, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if i % 2 == 1:\n",
    "                source.append(x)\n",
    "\n",
    "\n",
    "        for s, l, c in zip(source, self.loc, self.conf):\n",
    "            loc.append(l(s).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(s).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "       \n",
    "\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "        return loc, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "purple-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxEncoder(object):\n",
    "     '''\n",
    "     The corresponding anchors are generated by the size of the feature map. Because the multi-scale feature map is used in the ssd, \n",
    "     for the convenience of implementation, we usually normalize the generated anchor (cx, cy, w, h) to (0, 1) by the size of the feature map \n",
    "     where it is located\n",
    "     '''\n",
    "    def __init__(self, hparam):\n",
    "        self.variance = hparam.variance\n",
    "        default_boxes = list()\n",
    "        # Generate center co-ordinates\n",
    "        for k in range(len(hparam.grids)):\n",
    "            for v, u in itertools.product(range(hparam.grids[k]), repeat=2):\n",
    "                cx = (u + 0.5) * hparam.steps_512[k]\n",
    "                cy = (v + 0.5) * hparam.steps_512[k]\n",
    "                \n",
    "                s = hparam.sizes_512[k]\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = np.sqrt(hparam.sizes_512[k] * hparam.sizes_512[k + 1])\n",
    "                default_boxes.append((cx, cy, s, s))\n",
    "\n",
    "                s = hparam.sizes_512[k]\n",
    "                for ar in hparam.aspect_ratios[k]:\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s * np.sqrt(ar), s / np.sqrt(ar)))\n",
    "                    default_boxes.append(\n",
    "                        (cx, cy, s / np.sqrt(ar), s * np.sqrt(ar)))\n",
    "\n",
    "        default_boxes = np.clip(default_boxes, a_min=0, a_max=1)\n",
    "        self.default_boxes = np.array(default_boxes)\n",
    "\n",
    "    def encode(self, boxes, labels, threshold=0.5):\n",
    "       \n",
    "        if len(boxes) == 0:\n",
    "            return (\n",
    "                np.zeros(self.default_boxes.shape, dtype=np.float32),\n",
    "                np.zeros(self.default_boxes.shape[:1], dtype=np.int32))\n",
    "\n",
    "        iou = bbox_iou(point_form(self.default_boxes), boxes)\n",
    "\n",
    "        # Retrieve index having max iou\n",
    "        gt_idx = iou.argmax(axis=1)\n",
    "        iou = iou.max(axis=1)\n",
    "        boxes = boxes[gt_idx]\n",
    "        labels = labels[gt_idx]\n",
    "\n",
    "        loc = np.hstack((\n",
    "            ((boxes[:, :2] + boxes[:, 2:]) / 2 - self.default_boxes[:, :2]) /\n",
    "            (self.variance[0] * self.default_boxes[:, 2:]),\n",
    "            np.log((boxes[:, 2:] - boxes[:, :2]) / self.default_boxes[:, 2:]) /\n",
    "            self.variance[1]))\n",
    "\n",
    "        conf = 1 + labels\n",
    "        conf[iou < threshold] = 0\n",
    "       \n",
    "        return loc.astype(np.float32), conf.astype(np.int32)\n",
    "\n",
    "    def decode(self, loc):\n",
    "        \n",
    "        boxes = np.hstack((\n",
    "            self.default_boxes[:, :2] +\n",
    "            loc[:, :2] * self.variance[0] * self.default_boxes[:, 2:],\n",
    "            self.default_boxes[:, 2:] * np.exp(loc[:, 2:] * self.variance[1])))\n",
    "        boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "composite-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box_a, box_b):\n",
    "   \n",
    "    m = box_a.shape[0]\n",
    "    n = box_b.shape[0]\n",
    "    tl = np.maximum(box_a[:, None, :2], box_b[None, :, :2])\n",
    "    br = np.minimum(box_a[:, None, 2:], box_b[None, :, 2:])\n",
    "    wh = np.maximum(br-tl, 0)\n",
    "    inner = wh[:, :, 0]*wh[:, :, 1]\n",
    "    a = box_a[:, 2:] - box_a[:, :2]\n",
    "    b = box_b[:, 2:] - box_b[:, :2]\n",
    "    a = a[:, 0] * a[:, 1]\n",
    "    b = b[:, 0] * b[:, 1]\n",
    "    a = a[:, None]\n",
    "    b = b[None, :]\n",
    "    return inner / (a+b-inner)\n",
    "\n",
    "\n",
    "def nms(boxes, score, threshold=0.4):\n",
    "   \n",
    "    sort_ids = np.argsort(score)\n",
    "    pick = []\n",
    "    while len(sort_ids) > 0:\n",
    "        i = sort_ids[-1]\n",
    "        pick.append(i)\n",
    "        if len(sort_ids) == 1:\n",
    "            break\n",
    "        sort_ids = sort_ids[:-1]\n",
    "        box = boxes[i].reshape(1, 4)\n",
    "        ious = bbox_iou(box, boxes[sort_ids]).reshape(-1)\n",
    "        sort_ids = np.delete(sort_ids, np.where(ious > threshold)[0])\n",
    "\n",
    "    return pick\n",
    "\n",
    "def detect(locations, scores, nms_threshold, gt_threshold):\n",
    "   \n",
    "    scores = scores[:, 1:] \n",
    "    keep_boxes = []\n",
    "    keep_confs = []\n",
    "    keep_labels = []\n",
    "    \n",
    "    for i in range(scores.shape[1]):\n",
    "        mask = scores[:, i] >= gt_threshold\n",
    "        label_scores = scores[mask, i] \n",
    "        label_boxes = locations[mask]\n",
    "        if len(label_scores) == 0:\n",
    "            continue\n",
    "\n",
    "        pick = nms(label_boxes, label_scores, threshold=nms_threshold)\n",
    "        label_scores = label_scores[pick]\n",
    "        label_boxes = label_boxes[pick]\n",
    "        \n",
    "\n",
    "        keep_boxes.append(label_boxes.reshape(-1))\n",
    "        keep_confs.append(label_scores)\n",
    "        keep_labels.extend([i]*len(label_scores))\n",
    "    \n",
    "    if len(keep_boxes) == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "        \n",
    "    \n",
    "    keep_boxes = np.concatenate(keep_boxes, axis=0).reshape(-1, 4)\n",
    "\n",
    "    keep_confs = np.concatenate(keep_confs, axis=0)\n",
    "    keep_labels = np.array(keep_labels).reshape(-1)\n",
    "\n",
    "\n",
    "    return keep_boxes, keep_confs, keep_labels\n",
    "\n",
    "def point_form(boxes):\n",
    "    \n",
    "\n",
    "    tl = boxes[:, :2] - boxes[:, 2:]/2\n",
    "    br = boxes[:, :2] + boxes[:, 2:]/2\n",
    "\n",
    "    return np.concatenate([tl, br], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "coated-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negtives(logits, labels, pos, neg_ratio):\n",
    "    \n",
    "    \n",
    "    num_batch, num_anchors, num_classes = logits.shape\n",
    "    logits = logits.view(-1, num_classes)\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    losses = losses.view(num_batch, num_anchors)\n",
    "\n",
    "    losses[pos] = 0\n",
    "\n",
    "    \n",
    "    loss_idx = losses.argsort(1, descending=True)\n",
    "    rank = loss_idx.argsort(1) \n",
    "\n",
    "    num_pos = pos.long().sum(1, keepdim=True)\n",
    "    num_neg = torch.clamp(neg_ratio*num_pos, max=pos.shape[1]-1) #(batch, 1)\n",
    "    neg = rank < num_neg.expand_as(rank)\n",
    "    \n",
    "    return neg\n",
    "    \n",
    "class MultiBoxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, neg_ratio=3):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.neg_ratio = neg_ratio\n",
    "    \n",
    "    def forward(self, pred_loc, pred_label, gt_loc, gt_label):\n",
    "        num_batch = pred_loc.shape[0]\n",
    "\n",
    "        pos_idx = gt_label > 0\n",
    "        print('pred_loc.shape: ', pred_loc.shape)\n",
    "        print(\"pos_idx.shape: \", pos_idx.shape, 'pos_idx.unsqueeze(2).shape: ', pos_idx.unsqueeze(2).shape)\n",
    "        pos_loc_idx = pos_idx.unsqueeze(2).expand_as(pred_loc)\n",
    "        # Prediction Location Position\n",
    "        pred_loc_pos = pred_loc[pos_loc_idx].view(-1, 4)\n",
    "        # Ground Truth Location Position\n",
    "        gt_loc_pos = gt_loc[pos_loc_idx].view(-1, 4)\n",
    "\n",
    "        loc_loss = F.smooth_l1_loss(pred_loc_pos, gt_loc_pos, reduction='sum')\n",
    "\n",
    "        \n",
    "        logits = pred_label.detach()\n",
    "        labels = gt_label.detach()\n",
    "        neg_idx = hard_negtives(logits, labels, pos_idx, self.neg_ratio) #neg (batch, n)\n",
    "\n",
    "        pos_cls_mask = pos_idx.unsqueeze(2).expand_as(pred_label)\n",
    "        neg_cls_mask = neg_idx.unsqueeze(2).expand_as(pred_label)\n",
    "\n",
    "        conf_p = pred_label[(pos_cls_mask+neg_cls_mask).gt(0)].view(-1, self.num_classes)\n",
    "        target = gt_label[(pos_idx+neg_idx).gt(0)]\n",
    "\n",
    "        cls_loss = F.cross_entropy(conf_p, target, reduction='sum')\n",
    "        N = pos_idx.long().sum()\n",
    "\n",
    "        loc_loss /= N\n",
    "        cls_loss /= N\n",
    "\n",
    "        return loc_loss, cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mature-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter config\n",
    "class Config:\n",
    "    #class + 1\n",
    "    num_classes = 6\n",
    "    #learning rate\n",
    "    lr = 0.001\n",
    "    #ssd paper = 32\n",
    "    batch_size = 16 \n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "    # 40k + 10k = 116 epoc\n",
    "    epoch = 116 \n",
    "    #pre-train VGG root\n",
    "    #The resnet pre-train model is in lib.res-model...\n",
    "    save_folder = './weights/'\n",
    "    # basenet = 'vgg16_reducedfc.pth'\n",
    "    log_fn = 10 \n",
    "    neg_ratio = 3   \n",
    "    #input-image size\n",
    "    min_size = 512\n",
    "    #box out image size\n",
    "    grids = (38, 19, 10, 5, 3, 1)\n",
    "    #boxes num\n",
    "    # for ssd_300\n",
    "    # anchor_num = [4, 6, 6, 6, 4, 4]\n",
    "    # for ssd_512\n",
    "    anchor_num = [4, 6, 6, 6, 6, 4, 4]\n",
    "    #255 * R, G, B\n",
    "    mean = (104, 117, 123)\n",
    "    aspect_ratios = ((2,), (2, 3), (2, 3), (2, 3), (2,), (2,))\n",
    "    steps = [s / 300 for s in (8, 16, 32, 64, 100, 300)]\n",
    "    sizes = [s / 300 for s in (30, 60, 111, 162, 213, 264, 315)]\n",
    "    steps_512 = [s / 512 for s in (8, 16, 32, 64, 128, 256, 512)]\n",
    "    sizes_512 = [s / 512 for s in (20, 51, 133, 215, 296, 378, 460, 542)]\n",
    "    variance = (0.1, 0.2)\n",
    "\n",
    "hparam = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "preliminary-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def adjust_learning_rate1(optimizer):\n",
    "    lr = hparam.lr * 0.1\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def adjust_learning_rate2(optimizer):\n",
    "    lr = hparam.lr * 0.01\n",
    "    print('change learning rate, now learning rate is :', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "institutional-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train dataloader: 646\n"
     ]
    }
   ],
   "source": [
    "# df_train, df_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_amp = ObjDetDataset(X_train, y_train['bboxes'], y_train['classes'])\n",
    "val_amp = ObjDetDataset(X_val, y_val['bboxes'], y_val['classes'])\n",
    "\n",
    "model = RES18_SSD(hparam.num_classes, hparam.anchor_num, pretrained=False).to(device)\n",
    "model.train()\n",
    "\n",
    "mb = MultiBoxEncoder(hparam)\n",
    "\n",
    "# image_sets = [['2007', 'trainval'], ['2012', 'trainval']]\n",
    "# dataset = ObjDetection(df)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=hparam.batch_size, collate_fn=det_collate, num_workers=12)\n",
    "train_loader = torch.utils.data.DataLoader(train_amp, batch_size=hparam.batch_size, collate_fn=det_collate, num_workers=10)\n",
    "criterion = MultiBoxLoss(hparam.num_classes, hparam.neg_ratio).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hparam.lr, momentum=hparam.momentum,weight_decay=hparam.weight_decay)\n",
    "print(\"Number of items in train dataloader:\",len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "divided-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    for e in range(hparam.epoch):\n",
    "        if e == 77:\n",
    "            adjust_learning_rate1(optimizer)\n",
    "        elif e == 96:\n",
    "            adjust_learning_rate2(optimizer)\n",
    "        total_loc_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_loss = 0\n",
    "        for i , (img, boxes, labels) in enumerate(train_loader):\n",
    "            img = img.to(device)\n",
    "            gt_boxes = []\n",
    "            gt_labels = []\n",
    "            for j, box in enumerate(boxes):\n",
    "                # labels = box[:, 4]\n",
    "                label = labels[j]\n",
    "                # box = box[:, :-1]\n",
    "                match_loc, match_label = mb.encode(box, labels)\n",
    "            \n",
    "                gt_boxes.append(match_loc)\n",
    "                gt_labels.append(match_label)\n",
    "            \n",
    "            gt_boxes = torch.FloatTensor(gt_boxes).to(device)\n",
    "            gt_labels = torch.LongTensor(gt_labels).to(device)\n",
    "\n",
    "\n",
    "            p_loc, p_label = model(img)\n",
    "\n",
    "\n",
    "            loc_loss, cls_loss = criterion(p_loc, p_label, gt_boxes, gt_labels)\n",
    "\n",
    "            loss = loc_loss + cls_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loc_loss += loc_loss.item()\n",
    "            total_cls_loss += cls_loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % hparam.log_fn == 0:\n",
    "                avg_loc = total_loc_loss / (i+1)\n",
    "                avg_cls = total_cls_loss / (i+1)\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print('epoch[{}] | batch_idx[{}] | loc_loss [{:.2f}] | cls_loss [{:.2f}] | total_loss [{:.2f}]'.format(e, i, avg_loc, avg_cls, avg_loss))\n",
    "        if e > 100:\n",
    "            torch.save(model.state_dict(), os.path.join(hparam.save_folder, 'loss-{:.2f}.pth'.format(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "previous-genome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In forward: x.shape torch.Size([16, 3, 512, 512])\n",
      "In forward: x.shape torch.Size([16, 64, 256, 256])\n",
      "In forward: x.shape torch.Size([16, 64, 256, 256])\n",
      "In forward: x.shape torch.Size([16, 64, 256, 256])\n",
      "In forward: x.shape torch.Size([16, 64, 128, 128])\n",
      "In forward: x.shape torch.Size([16, 64, 128, 128])\n",
      "In forward: x.shape torch.Size([16, 128, 64, 64])\n",
      "In forward: x.shape torch.Size([16, 256, 32, 32])\n",
      "In forward: x.shape torch.Size([16, 512, 16, 16])\n",
      "In forward: x.shape torch.Size([16, 512, 16, 16])\n",
      "In forward: x.shape torch.Size([16, 1024, 16, 16])\n",
      "In forward: x.shape torch.Size([16, 1024, 16, 16])\n",
      "In forward: x.shape torch.Size([16, 1024, 16, 16])\n",
      "pred_loc.shape:  torch.Size([16, 24656, 4])\n",
      "pos_idx.shape:  torch.Size([16, 8732]) pos_idx.unsqueeze(2).shape:  torch.Size([16, 8732, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (24656) must match the existing size (8732) at non-singleton dimension 1.  Target sizes: [16, 24656, 4].  Tensor sizes: [16, 8732, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-0730e4338b94>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-1dbef7790008>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred_loc, pred_label, gt_loc, gt_label)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred_loc.shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_loc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pos_idx.shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_idx.unsqueeze(2).shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mpos_loc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mpred_loc_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_loc_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mgt_loc_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_loc_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (24656) must match the existing size (8732) at non-singleton dimension 1.  Target sizes: [16, 24656, 4].  Tensor sizes: [16, 8732, 1]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_loader)What are your top three takeaways from this session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , (img, boxes, labels) in enumerate(train_loader):\n",
    "    # img = img.to(device)\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for j, box in enumerate(boxes):\n",
    "        # labels = box[:, 4]\n",
    "        label = labels[j]\n",
    "        # box = box[:, :-1]\n",
    "        print(\"box\", box)\n",
    "        match_loc, match_label = mb.encode(box, labels)\n",
    "        print(\"labels\", labels)\n",
    "        gt_boxes.append(match_loc)\n",
    "        gt_labels.append(match_label)\n",
    "        print(\"mb.encode:\\n\", match_loc, match_label)\n",
    "    # gt_boxes = torch.FloatTensor(gt_boxes).to(device)\n",
    "    # gt_labels = torch.LongTensor(gt_labels).to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "outdoor-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 512, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.empty((16,512, 512, 3), dtype=torch.int32, device = 'cuda')\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "absolute-shock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(a, 3, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "alien-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sitting-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 14 18:39:54 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3080    Off  | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   54C    P8    31W / 320W |   1493MiB / 10014MiB |     13%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
